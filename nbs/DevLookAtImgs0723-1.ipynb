{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 224\n"
     ]
    }
   ],
   "source": [
    "SEED = 111\n",
    "\n",
    "BS = 9\n",
    "FP16 = True\n",
    "PERC_VAL = 0.2\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b0'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "params_tfms = dict(\n",
    "    max_zoom=1,\n",
    "    max_warp=0,\n",
    "    max_lighting=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "\n",
    "import cv2\n",
    "\n",
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True \n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "        quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image, _ = crop_margin(image)\n",
    "\n",
    "    image = center_crop(image)\n",
    "\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) ,-4 ,128)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "\n",
    "vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 9164), (2, 2515), (1, 1046), (4, 496), (3, 441)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = '../input/diabetic-retinopathy-detection'\n",
    "pp = Path(p)\n",
    "\n",
    "train=pd.read_csv(pp/'trainLabels.csv')\n",
    "train=pd.read_csv(pp/'trainLabels.csv').sample(10000)\n",
    "img2grd_diab_train=[(f'../input/diabetic-retinopathy-detection/train_images/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab_train\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())\n",
    "\n",
    "# test=pd.read_csv(pp/'retinopathy_solution.csv')\n",
    "# img2grd_diab_test=[(f'../input/diabetic-retinopathy-detection/test_images/{o[0]}.jpeg',o[1],'diab')  for o in test.values]\n",
    "# img2grd += img2grd_diab_test\n",
    "# display(len(img2grd))\n",
    "# display(Counter(o[1] for o in img2grd).most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14075"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 9298), (2, 2651), (1, 1066), (4, 545), (3, 515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14178"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 9332), (2, 2683), (1, 1071), (4, 558), (3, 534)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = '../input/IDRID/B. Disease Grading'\n",
    "pp = Path(p)\n",
    "\n",
    "train=pd.read_csv(pp/'2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv')\n",
    "img2grd_idrid_train=[(f'../input/IDRID/B. Disease Grading/1. Original Images/a. Training Set/{o[0]}.jpg',o[1],'idrid')  for o in train.values]\n",
    "img2grd += img2grd_idrid_train\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())\n",
    "\n",
    "test=pd.read_csv(pp/'2. Groundtruths/b. IDRiD_Disease Grading_Testing Labels.csv')\n",
    "img2grd_idrid_test=[(f'../input/IDRID/B. Disease Grading/1. Original Images/b. Testing Set/{o[0]}.jpg',o[1],'idrid')  for o in test.values]\n",
    "img2grd += img2grd_idrid_test\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14178, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select val set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8888/notebooks/data/git/blnd/nbs/DevLookSubmDistr.ipynb#\n",
    "\n",
    "    2    1050\n",
    "    3     373\n",
    "    0     321\n",
    "    1     138\n",
    "    4      46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.5128205128205128,\n",
       " 3: 0.20512820512820512,\n",
       " 0: 0.1794871794871795,\n",
       " 1: 0.07692307692307693,\n",
       " 4: 0.02564102564102564}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2c = {2:    1000,\n",
    "3:     400,\n",
    "0:     350,\n",
    "1:     150,\n",
    "4:      50,}\n",
    "sm = sum(t2c.values())\n",
    "t2p = {t:c/sm for t,c in t2c.items()}\n",
    "t2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 375, 3: 150, 0: 131, 1: 56, 4: 18}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2c_val = {t:int(p*len_blnd*PERC_VAL) for t,p in t2p.items()}\n",
    "t2c_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "idx_blnd_train = np.where(df.src=='blnd')[0]\n",
    "\n",
    "idx_val = []\n",
    "for t,c_val in t2c_val.items():\n",
    "    idx_t_all = np.where(df.target==t)[0]\n",
    "    idx2choosefrom = list(set(idx_t_all) & set(idx_blnd_train))\n",
    "    idx_t = np.random.choice(idx2choosefrom, c_val, replace=False)\n",
    "    idx_val += list(idx_t)\n",
    "# idx_val = np.random.choice(idx_blnd_train, int(len_blnd*0.10), replace=False)\n",
    "\n",
    "df['is_val']=False\n",
    "df.loc[idx_val, 'is_val']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    13448\n",
       "True       730\n",
       "Name: is_val, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    375\n",
       "3    150\n",
       "0    131\n",
       "1     56\n",
       "4     18\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.is_val, 'target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 1.26 s, total: 17.6 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz, bs):\n",
    "    src = (ImageList.from_df(df=df,path='./',cols='fnm') \n",
    "            .split_from_df(col='is_val') \n",
    "            .label_from_df(cols='target',  \n",
    "                           label_cls=FloatList)\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=SZ, \n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "bs = BS \n",
    "sz = SZ\n",
    "set_torch_seed()\n",
    "data = get_data(sz, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 41, in _pin_memory_loop\n    batch = pin_memory_batch(batch)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 58, in pin_memory_batch\n    return [pin_memory_batch(sample) for sample in batch]\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 58, in <listcomp>\n    return [pin_memory_batch(sample) for sample in batch]\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 50, in pin_memory_batch\n    return batch.pin_memory()\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/THCCachingHostAllocator.cpp:265\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a8ee2cb5a0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data.show_batch(rows=3, figsize=(10, 8))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, rows, ds_type, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;34m\"Show a batch of data in `ds_type` on a few `rows`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_square_show\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, ds_type, detach, denorm, cpu)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 41, in _pin_memory_loop\n    batch = pin_memory_batch(batch)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 58, in pin_memory_batch\n    return [pin_memory_batch(sample) for sample in batch]\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 58, in <listcomp>\n    return [pin_memory_batch(sample) for sample in batch]\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 50, in pin_memory_batch\n    return batch.pin_memory()\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/THCCachingHostAllocator.cpp:265\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use test.sample to see different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test.sample(9),\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 8), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
