{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = f'RndMdl0814_2_seed{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'\n",
    "\n",
    "# p_o = f'.'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = False\n",
    "if dbg: dbgsz=500\n",
    "\n",
    "from fastai.vision import * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "source": [
    "!pip install ../input/efficientnetpytorch/efficientnet_pytorch-0.3.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 15 11:37:13 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    41W / 300W |     10MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading: \"http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\" to /tmp/.cache/torch/checkpoints/efficientnet-b3-c8376fa2.pth\n",
    "import os\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "\n",
    "!cp ../input/efficientnetpytorch/*.pth /tmp/.cache/torch/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 456\n"
     ]
    }
   ],
   "source": [
    "BS = 16\n",
    "FP16 = True\n",
    "PERC_VAL = 0.1\n",
    "WD = 0.01\n",
    "\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_open_yz = True\n",
    "\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "def load_ben_color(fn)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = crop_image_from_gray(image)\n",
    "    image, _ = crop_margin(image)\n",
    "    image = center_crop(image)\n",
    "    image = cv2.resize(image, (640, 480))#most common in test\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) , -4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfms = dict(\n",
    "    do_flip=True,\n",
    "    flip_vert=True,\n",
    "    max_rotate=360,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "    \n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "    \n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = load_ben_color(fn)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "    \n",
    "if use_open_yz:\n",
    "    vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "#         quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        quadratic_weighted_kappa(np.argmax(y_pred,1), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    aug_tfms = [o for o in learn.data.train_ds.tfms if o.tfm !=zoom]\n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "t2c_trn_has = dict(cnt)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/diabetic-retinopathy-resized'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'trainLabels.csv')\n",
    "img2grd_diab = [(f'{p}/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "# img2grd_diab = [(f'{p}/resized_train/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38788, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14725, '../input/diabetic-retinopathy-resized/resized_train/13896_right.jpeg', 0, 'diab'],\n",
       "       [11804, '../input/diabetic-retinopathy-resized/resized_train/10206_left.jpeg', 0, 'diab'],\n",
       "       [18694, '../input/diabetic-retinopathy-resized/resized_train/18827_left.jpeg', 0, 'diab'],\n",
       "       [30763, '../input/diabetic-retinopathy-resized/resized_train/34302_right.jpeg', 1, 'diab'],\n",
       "       [27809, '../input/diabetic-retinopathy-resized/resized_train/30516_right.jpeg', 0, 'diab']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27615\n",
       "2     6291\n",
       "1     2813\n",
       "3     1066\n",
       "4     1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use = df[df.src=='blnd'].copy()\n",
    "\n",
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 737, 3: 647, 4: 408, 1: 300}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randint(low=300, high=900):\n",
    "    res = np.random.randn()*300+600\n",
    "    return int(min(max(low, res), high))\n",
    "\n",
    "# set_torch_seed()\n",
    "n_t_extra = {2:get_randint(),3:get_randint(),4:get_randint(),1:get_randint()}\n",
    "n_t_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "for t,n in n_t_extra.items():\n",
    "    df_t_diab = df[(df.target==t) & (df.src=='diab')]\n",
    "    df2use = pd.concat([df2use, df_t_diab.sample(min(n, len(df_t_diab)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5754, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2    1736\n",
       "3     840\n",
       "4     703\n",
       "1     670\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: \n",
    "    df2use = df2use.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.68 s, sys: 179 ms, total: 4.86 s\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz=SZ, bs=BS):\n",
    "    src = (ImageList.from_df(df=df2use,path='./',cols='fnm') \n",
    "#             .split_by_rand_pct(0.2) \n",
    "            .split_none()\n",
    "            .label_from_df(cols='target',  \n",
    "                           #label_cls=FloatList\n",
    "                          )\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=sz,\n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "\n",
    "set_torch_seed()\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: test = test.head(dbgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test,\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME, num_classes=5) \n",
    "learn = Learner(data, model, path=p_o, \n",
    "#                 wd=WD,  \n",
    "#                 metrics=[accuracy, qwk],\n",
    "               )\n",
    "if FP16: learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "learn.recorder.plot(suggestion=True, skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.872972</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.835260</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.832703</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.729465</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.701804</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.610255</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.587726</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.547529</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.528572</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "learn.fit_one_cycle(10, max_lr=1e-3, \n",
    "#                     callbacks=[SaveModelCallback(learn, \n",
    "#                                                  every='epoch', \n",
    "#                                                  name=f'{PRFX}_model')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'rndmdl_seed_{SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 4.47 s, total: 17.2 s\n",
      "Wall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds_tst = preds_tst.numpy().squeeze()\n",
    "preds_tst = np.argmax(preds_tst, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst_tta, _ = learn.TTA(ds_type=DatasetType.Test)\n",
    "preds_tst_tta = preds_tst_tta.numpy().squeeze()\n",
    "preds_tst_tta = np.argmax(preds_tst_tta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1239\n",
       "0     364\n",
       "1     139\n",
       "4     109\n",
       "3      77\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds_tst.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.Series(preds_tst_tta.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          4\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "subm['diagnosis'] = preds_tst\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1239\n",
       "0     364\n",
       "1     139\n",
       "4     109\n",
       "3      77\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
