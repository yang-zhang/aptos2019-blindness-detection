{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/abhishek/very-simple-pytorch-training-0-59?scriptVersionId=16436961\n",
    "- https://www.kaggle.com/abhishek/pytorch-inference-kernel-lazy-tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = True\n",
    "if dbg:\n",
    "    dbgtrnsz=15000\n",
    "    dbgvalsz=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = 'devCv070115'\n",
    "SEED = 111\n",
    "SZ = (256, 256)\n",
    "BSZ = 112\n",
    "BSZ_INFER = BSZ*2\n",
    "N_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True \n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "p_o = f'../output/{PRFX}'\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2grd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "\n",
    "len_blnd = len(train)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1])  for o in train.values]\n",
    "\n",
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38788"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 27615), (2, 6291), (1, 2813), (3, 1066), (4, 1003)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 67148), (2, 14152), (1, 6575), (3, 2280), (4, 2209)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = '../input/diabetic-retinopathy-detection'\n",
    "pp = Path(p)\n",
    "train=pd.read_csv(pp/'trainLabels.csv')\n",
    "test=pd.read_csv(pp/'retinopathy_solution.csv')\n",
    "\n",
    "img2grd_diab_train=[(f'{p}/train_images/{o[0]}.jpeg',o[1])  for o in train.values]\n",
    "img2grd_diab_test=[(f'{p}/test_images/{o[0]}.jpeg',o[1])  for o in test.values]\n",
    "img2grd += img2grd_diab_train\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())\n",
    "img2grd += img2grd_diab_test\n",
    "len(img2grd)\n",
    "display(Counter(o[1] for o in img2grd).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92777"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 67282), (2, 14288), (1, 6595), (3, 2354), (4, 2258)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 67316), (2, 14320), (1, 6600), (3, 2373), (4, 2271)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = '../input/IDRID/B. Disease Grading'\n",
    "pp = Path(p)\n",
    "train=pd.read_csv(pp/'2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv')\n",
    "test=pd.read_csv(pp/'2. Groundtruths/b. IDRiD_Disease Grading_Testing Labels.csv')\n",
    "\n",
    "img2grd_idrid_train=[(f'{p}/1. Original Images/a. Training Set/{o[0]}.jpg',o[1])  for o in train.values]\n",
    "img2grd_idrid_test=[(f'{p}/1. Original Images/b. Testing Set/{o[0]}.jpg',o[1])  for o in test.values]\n",
    "img2grd += img2grd_idrid_train\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())\n",
    "img2grd += img2grd_idrid_test\n",
    "len(img2grd)\n",
    "display(Counter(o[1] for o in img2grd).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2grd = np.array(img2grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are here!\n"
     ]
    }
   ],
   "source": [
    "if np.all([Path(o[0]).exists() for o in img2grd]): print('All files are here!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([['../input/diabetic-retinopathy-detection/train_images/10_left.jpeg',\n",
       "         '0'],\n",
       "        ['../input/diabetic-retinopathy-detection/train_images/10_right.jpeg',\n",
       "         '0'],\n",
       "        ['../input/diabetic-retinopathy-detection/train_images/13_left.jpeg',\n",
       "         '0']], dtype='<U82'),\n",
       " array([['../input/aptos2019-blindness-detection/train_images/000c1434d8d7.png',\n",
       "         '2'],\n",
       "        ['../input/aptos2019-blindness-detection/train_images/001639a390f0.png',\n",
       "         '4'],\n",
       "        ['../input/aptos2019-blindness-detection/train_images/0024cdab0c1e.png',\n",
       "         '1']], dtype='<U82'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "idx_val = range(len_blnd)\n",
    "idx_trn = range(len_blnd, len(img2grd))\n",
    "\n",
    "img2grd_trn = img2grd[idx_trn]\n",
    "img2grd_val = img2grd[idx_val]\n",
    "\n",
    "display(len(img2grd_trn), len(img2grd_val))\n",
    "\n",
    "img2grd_trn[:3], img2grd_val[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg:\n",
    "    img2grd_trn = img2grd_trn[:dbgtrnsz]\n",
    "    img2grd_val = img2grd_val[:dbgvalsz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlndDataset(Dataset):\n",
    "    def __init__(self, img2grd, transform):\n",
    "        self.img2grd = img2grd\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img2grd)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img,grd = self.img2grd[idx]\n",
    "        image = self.transform(Image.open(img))\n",
    "        label = torch.tensor(int(grd))\n",
    "        return image, label\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(SZ),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ds_trn = BlndDataset(img2grd_trn, transform=transform_train)\n",
    "ds_val = BlndDataset(img2grd_val, transform=transform_train)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(ds_trn, batch_size=BSZ, shuffle=True, num_workers=0)\n",
    "data_loader_val = torch.utils.data.DataLoader(ds_val, batch_size=BSZ_INFER, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"../input/pytorch_models/resnet50-19c8e357.pth\"));"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fc = nn.Linear(2048, 1)\n",
    "model.fc = nn.Sequential(\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.25),\n",
    "                          nn.Linear(in_features=2048, out_features=2048, bias=True),\n",
    "                          nn.ReLU(),\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.5),\n",
    "                          nn.Linear(in_features=2048, out_features=1, bias=True),\n",
    "                         )\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = [\n",
    "         {'params': model.layer4.parameters(), 'lr': 1e-4, 'weight': 0.001},\n",
    "         {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "         ]\n",
    "\n",
    "optimizer = optim.Adam(plist, lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "[2019-07-01 21:12:16.402695] epoch-0 step-0/134 loss: 1.50695\n",
      "[2019-07-01 21:15:20.366051] epoch-0 step-10/134 loss: 1.73117\n",
      "[2019-07-01 21:18:25.876056] epoch-0 step-20/134 loss: 1.53160\n",
      "[2019-07-01 21:21:27.559572] epoch-0 step-30/134 loss: 1.34063\n",
      "[2019-07-01 21:24:30.332973] epoch-0 step-40/134 loss: 1.21552\n",
      "[2019-07-01 21:27:33.280219] epoch-0 step-50/134 loss: 1.13192\n",
      "[2019-07-01 21:30:37.132392] epoch-0 step-60/134 loss: 1.06562\n",
      "[2019-07-01 21:33:41.272118] epoch-0 step-70/134 loss: 1.01162\n",
      "[2019-07-01 21:36:39.340381] epoch-0 step-80/134 loss: 0.97318\n",
      "[2019-07-01 21:39:44.769561] epoch-0 step-90/134 loss: 0.94090\n",
      "[2019-07-01 21:42:52.124400] epoch-0 step-100/134 loss: 0.91363\n",
      "[2019-07-01 21:45:54.849619] epoch-0 step-110/134 loss: 0.89589\n",
      "[2019-07-01 21:48:58.159468] epoch-0 step-120/134 loss: 0.87528\n",
      "[2019-07-01 21:51:57.447107] epoch-0 step-130/134 loss: 0.86079\n",
      "[2019-07-01 21:53:19.806678] val step-0/17\n",
      "[2019-07-01 21:57:58.031931] val step-10/17\n",
      "Training Loss: 0.8590; Val Loss: 0.7022\n",
      "Epoch 1/1\n",
      "[2019-07-01 22:00:40.714669] epoch-1 step-0/134 loss: 0.63470\n",
      "[2019-07-01 22:03:43.920141] epoch-1 step-10/134 loss: 0.56222\n",
      "[2019-07-01 22:06:45.070559] epoch-1 step-20/134 loss: 0.56462\n",
      "[2019-07-01 22:09:49.500670] epoch-1 step-30/134 loss: 0.57357\n",
      "[2019-07-01 22:12:49.151502] epoch-1 step-40/134 loss: 0.57338\n",
      "[2019-07-01 22:15:53.859989] epoch-1 step-50/134 loss: 0.55802\n",
      "[2019-07-01 22:18:54.067816] epoch-1 step-60/134 loss: 0.54849\n",
      "[2019-07-01 22:21:53.587291] epoch-1 step-70/134 loss: 0.55566\n",
      "[2019-07-01 22:24:54.968610] epoch-1 step-80/134 loss: 0.56260\n",
      "[2019-07-01 22:28:00.288113] epoch-1 step-90/134 loss: 0.56161\n",
      "[2019-07-01 22:31:01.904407] epoch-1 step-100/134 loss: 0.57006\n",
      "[2019-07-01 22:34:02.976793] epoch-1 step-110/134 loss: 0.56779\n",
      "[2019-07-01 22:37:04.745001] epoch-1 step-120/134 loss: 0.56722\n",
      "[2019-07-01 22:40:05.973226] epoch-1 step-130/134 loss: 0.56533\n",
      "[2019-07-01 22:41:28.406410] val step-0/17\n",
      "[2019-07-01 22:46:07.421411] val step-10/17\n",
      "Training Loss: 0.5643; Val Loss: 0.9911\n",
      "Training complete in 96m 34s\n"
     ]
    }
   ],
   "source": [
    "len_dl = len(data_loader)\n",
    "len_ds = len(ds_trn)\n",
    "len_dl_val = len(data_loader_val)\n",
    "y_val = np.array([int(o[1]) for o in ds_val.img2grd])[:,None]\n",
    "\n",
    "since = time.time()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "set_torch_seed()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Epoch {epoch}/{N_EPOCHS-1}')\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_n = 0\n",
    "    for step, d in enumerate(data_loader):\n",
    "        inputs = d[0]\n",
    "        labels = d[1].view(-1, 1)\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_n += inputs.size(0)\n",
    "        if (step) % (10 if dbg else 100) == 0: \n",
    "            print(f'[{datetime.datetime.now()}] epoch-{epoch} step-{step}/{len_dl} loss: {running_loss/running_n:.5f}')\n",
    "    epoch_loss = running_loss / len_ds\n",
    "    \n",
    "    ###### val #######\n",
    "    model.eval()\n",
    "    preds_val = np.zeros((len(ds_val), 1))\n",
    "    for step, d in enumerate(data_loader_val):\n",
    "        if (step) % (10 if dbg else 100) == 0: \n",
    "            print(f'[{datetime.datetime.now()}] val step-{step}/{len_dl_val}')\n",
    "        inputs = d[0]\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        with torch.no_grad(): outputs = model(inputs)\n",
    "        preds_val[step*BSZ_INFER:(step+1)*BSZ_INFER] = outputs.detach().cpu().squeeze().numpy()[:,None]#.ravel().reshape(-1, 1)\n",
    "    \n",
    "    mse_val = mean_squared_error(preds_val, y_val)        \n",
    "    print(f'Training Loss: {epoch_loss:.4f}; Val Loss: {mse_val:.4f}')\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 112 ms, total: 143 ms\n",
      "Wall time: 306 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch.save(model.state_dict(), f\"{p_o}/model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=False)\n",
    "# model.fc = nn.Linear(2048, 1)\n",
    "model.fc = nn.Sequential(\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.25),\n",
    "                          nn.Linear(in_features=2048, out_features=2048, bias=True),\n",
    "                          nn.ReLU(),\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.5),\n",
    "                          nn.Linear(in_features=2048, out_features=1, bias=True),\n",
    "                         )\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(f\"{p_o}/model.bin\"));\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-01 22:49:02.886372] val step-0/17\n",
      "[2019-07-01 22:53:41.317118] val step-10/17\n",
      "Val Loss: 0.9898\n"
     ]
    }
   ],
   "source": [
    "y_val = np.array([int(o[1]) for o in ds_val.img2grd])[:,None]\n",
    "len_dl_val = len(data_loader_val)\n",
    "model.eval()\n",
    "preds_val = np.zeros((len(ds_val), 1))\n",
    "for step, d in enumerate(data_loader_val):\n",
    "    if (step) % (10 if dbg else 100) == 0: \n",
    "        print(f'[{datetime.datetime.now()}] val step-{step}/{len_dl_val}')\n",
    "    inputs = d[0]\n",
    "    inputs = inputs.to(device, dtype=torch.float)\n",
    "    with torch.no_grad(): outputs = model(inputs)\n",
    "    preds_val[step*BSZ_INFER:(step+1)*BSZ_INFER] = outputs.detach().cpu().squeeze().numpy()[:,None]#.ravel().reshape(-1, 1)\n",
    "mse_val = mean_squared_error(preds_val, y_val)        \n",
    "print(f'Val Loss: {mse_val:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/88773#latest-515044\n",
    "# We used OptimizedRounder given by hocop1. https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107#480970\n",
    "# put numerical value to one of bins\n",
    "def to_bins(x, borders):\n",
    "    for i in range(len(borders)):\n",
    "        if x <= borders[i]:\n",
    "            return i\n",
    "    return len(borders)\n",
    "\n",
    "class Hocop1OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _loss(self, coef, X, y, idx):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        ll = -quadratic_weighted_kappa(y, X_p)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        coef = [1.5, 2.0, 2.5, 3.0]\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n",
    "        for it1 in range(10):\n",
    "            for idx in range(4):\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                coef[idx] = a\n",
    "                la = self._loss(coef, X, y, idx)\n",
    "                coef[idx] = b\n",
    "                lb = self._loss(coef, X, y, idx)\n",
    "                for it in range(20):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        coef[idx] = a\n",
    "                        la = self._loss(coef, X, y, idx)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        coef[idx] = b\n",
    "                        lb = self._loss(coef, X, y, idx)\n",
    "        self.coef_ = {'x': coef}\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107#480970\n",
    "class AbhishekOptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket(preds_raw, coef = [0.5, 1.5, 2.5, 3.5]):\n",
    "    preds = np.zeros(preds_raw.shape)\n",
    "    for i, pred in enumerate(preds_raw):\n",
    "        if pred < coef[0]:\n",
    "            preds[i] = 0\n",
    "        elif pred >= coef[0] and pred < coef[1]:\n",
    "            preds[i] = 1\n",
    "        elif pred >= coef[1] and pred < coef[2]:\n",
    "            preds[i] = 2\n",
    "        elif pred >= coef[2] and pred < coef[3]:\n",
    "            preds[i] = 3\n",
    "        else:\n",
    "            preds[i] = 4\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2coefs = {'simple': [0.5, 1.5, 2.5, 3.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 0 ns, total: 13.6 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optR = Hocop1OptimizedRounder()\n",
    "optR.fit(preds_val, y_val)\n",
    "optnm2coefs['hocop1'] = optR.coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.94 s, sys: 0 ns, total: 4.94 s\n",
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optR = AbhishekOptimizedRounder()\n",
    "optR.fit(preds_val, y_val)\n",
    "optnm2coefs['abhishek'] = optR.coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'simple': [0.5, 1.5, 2.5, 3.5],\n",
       " 'hocop1': [1.1590974035597121,\n",
       "  1.6327897209636173,\n",
       "  2.514047719109641,\n",
       "  3.4950440745572],\n",
       " 'abhishek': array([0.91589737, 1.54972675, 2.88928779, 5.04515948])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optnm2coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2preds_val_grd = {k: bucket(preds_val, coef) for k,coef in optnm2coefs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2qwk = {k: quadratic_weighted_kappa(y_val, preds) for k,preds in optnm2preds_val_grd.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'simple': 0.7401184102151017,\n",
       " 'hocop1': 0.7848234985186533,\n",
       " 'abhishek': 0.8059010511979416}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optnm2qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_val.squeeze()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2058984161660296"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optnm2preds_val_grd['abhishek'].squeeze().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1659), (3.0, 798), (2.0, 697), (1.0, 468), (4.0, 40)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(optnm2preds_val_grd['abhishek'].squeeze()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.0, 2),\n",
       " (3.0, 4),\n",
       " (0.0, 1),\n",
       " (1.0, 0),\n",
       " (1.0, 0),\n",
       " (3.0, 4),\n",
       " (0.0, 0),\n",
       " (3.0, 2),\n",
       " (1.0, 2),\n",
       " (0.0, 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(optnm2preds_val_grd['abhishek'].squeeze(), y_val.squeeze()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5895685417804478"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(optnm2preds_val_grd['abhishek'].squeeze()== y_val.squeeze()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(optnm2qwk, open(f'{p_o}/optnm2qwk.p', 'wb'))\n",
    "pickle.dump(optnm2coefs, open(f'{p_o}/optnm2coefs.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = 'devCv0701'\n",
    "SEED = 111\n",
    "SZ = (256, 256)\n",
    "BSZ = 112\n",
    "BSZ_INFER = BSZ*2\n",
    "N_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from pathlib import Path\n",
    "p_o = f'../output/{PRFX}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../input/aptos2019-blindness-detection/test_images/0005cfc8afb6.png', -1),\n",
       " ('../input/aptos2019-blindness-detection/test_images/003f0afdcd15.png', -1),\n",
       " ('../input/aptos2019-blindness-detection/test_images/006efc72b638.png', -1),\n",
       " ('../input/aptos2019-blindness-detection/test_images/00836aaacf06.png', -1),\n",
       " ('../input/aptos2019-blindness-detection/test_images/009245722fa4.png', -1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test = pd.read_csv(pp/'test.csv')\n",
    "\n",
    "img2grd_tst = [(f'{p}/test_images/{o[0]}.png', -1)  for o in test.values]\n",
    "img2grd_tst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlndDataset(Dataset):\n",
    "    def __init__(self, img2grd, transform):\n",
    "        self.img2grd = img2grd\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img2grd)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img,grd = self.img2grd[idx]\n",
    "        image = self.transform(Image.open(img))\n",
    "        label = torch.tensor(int(grd))\n",
    "        return image, label\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(SZ),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ds_tst = BlndDataset(img2grd_tst, transform=transform_test)\n",
    "data_loader_tst = torch.utils.data.DataLoader(ds_tst, batch_size=BSZ_INFER, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=False)\n",
    "# model.fc = nn.Linear(2048, 1)\n",
    "model.fc = nn.Sequential(\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.25),\n",
    "                          nn.Linear(in_features=2048, out_features=2048, bias=True),\n",
    "                          nn.ReLU(),\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.5),\n",
    "                          nn.Linear(in_features=2048, out_features=1, bias=True),\n",
    "                         )\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(f\"{p_o}/model.bin\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run predicting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-01 22:56:33.845588] step-0/9\n",
      "Test complete in 1m 20s\n"
     ]
    }
   ],
   "source": [
    "len_dl_tst = len(data_loader_tst)\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "preds_tst = np.zeros((len(ds_tst), 1))\n",
    "for step, d in enumerate(data_loader_tst):\n",
    "    if (step) % 10 == 0: \n",
    "        print(f'[{datetime.datetime.now()}] step-{step}/{len_dl_tst}')\n",
    "    inputs = d[0]\n",
    "    inputs = inputs.to(device, dtype=torch.float)\n",
    "    with torch.no_grad(): outputs = model(inputs)\n",
    "    preds_tst[step*BSZ_INFER:(step+1)*BSZ_INFER] = outputs.detach().cpu().squeeze().numpy()[:,None]#.ravel().reshape(-1, 1)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Test complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94371623],\n",
       "       [2.55177855],\n",
       "       [2.37508798],\n",
       "       ...,\n",
       "       [2.49047399],\n",
       "       [3.41678333],\n",
       "       [2.10848808]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8143325139298896"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_tst.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket(preds_raw, coef = [0.5, 1.5, 2.5, 3.5]):\n",
    "    preds = np.zeros(preds_raw.shape)\n",
    "    for i, pred in enumerate(preds_raw):\n",
    "        if pred < coef[0]:\n",
    "            preds[i] = 0\n",
    "        elif pred >= coef[0] and pred < coef[1]:\n",
    "            preds[i] = 1\n",
    "        elif pred >= coef[1] and pred < coef[2]:\n",
    "            preds[i] = 2\n",
    "        elif pred >= coef[2] and pred < coef[3]:\n",
    "            preds[i] = 3\n",
    "        else:\n",
    "            preds[i] = 4\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'simple': 0.7401184102151017,\n",
       "  'hocop1': 0.7848234985186533,\n",
       "  'abhishek': 0.8059010511979416},\n",
       " {'simple': [0.5, 1.5, 2.5, 3.5],\n",
       "  'hocop1': [1.1590974035597121,\n",
       "   1.6327897209636173,\n",
       "   2.514047719109641,\n",
       "   3.4950440745572],\n",
       "  'abhishek': array([0.91589737, 1.54972675, 2.88928779, 5.04515948])})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optnm2qwk, optnm2coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = optnm2coefs['simple']\n",
    "preds_tst_grd = bucket(preds_tst, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3., 2., ..., 2., 3., 2.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_tst_grd.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.0, 986), (1.0, 558), (3.0, 270), (0.0, 76), (4.0, 38)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(preds_tst_grd.squeeze()).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          1\n",
       "1  003f0afdcd15          3\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "sample.diagnosis = preds_tst_grd.squeeze().astype(int)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
