{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = f'RndMdl0814_2_seed{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'\n",
    "\n",
    "# p_o = f'.'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = False\n",
    "if dbg: dbgsz=500\n",
    "\n",
    "from fastai.vision import * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "source": [
    "!pip install ../input/efficientnetpytorch/efficientnet_pytorch-0.3.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 15 13:19:00 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   46C    P0    42W / 300W |     10MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading: \"http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\" to /tmp/.cache/torch/checkpoints/efficientnet-b3-c8376fa2.pth\n",
    "import os\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "\n",
    "!cp ../input/efficientnetpytorch/*.pth /tmp/.cache/torch/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 456\n"
     ]
    }
   ],
   "source": [
    "BS = 16\n",
    "FP16 = True\n",
    "PERC_VAL = 0.1\n",
    "WD = 0.01\n",
    "\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_open_yz = True\n",
    "\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "def load_ben_color(fn)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = crop_image_from_gray(image)\n",
    "    image, _ = crop_margin(image)\n",
    "    image = center_crop(image)\n",
    "    image = cv2.resize(image, (640, 480))#most common in test\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) , -4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfms = dict(\n",
    "    do_flip=True,\n",
    "    flip_vert=True,\n",
    "    max_rotate=360,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "    \n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "    \n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = load_ben_color(fn)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "    \n",
    "if use_open_yz:\n",
    "    vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "#         quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        quadratic_weighted_kappa(np.argmax(y_pred,1), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    aug_tfms = [o for o in learn.data.train_ds.tfms if o.tfm !=zoom]\n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "t2c_trn_has = dict(cnt)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/diabetic-retinopathy-resized'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'trainLabels.csv')\n",
    "img2grd_diab = [(f'{p}/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "# img2grd_diab = [(f'{p}/resized_train/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38788, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6987, '../input/diabetic-retinopathy-resized/resized_train/4155_right.jpeg', 4, 'diab'],\n",
       "       [20720, '../input/diabetic-retinopathy-resized/resized_train/21427_left.jpeg', 0, 'diab'],\n",
       "       [25914, '../input/diabetic-retinopathy-resized/resized_train/28047_left.jpeg', 3, 'diab'],\n",
       "       [32150, '../input/diabetic-retinopathy-resized/resized_train/36057_left.jpeg', 0, 'diab'],\n",
       "       [36053, '../input/diabetic-retinopathy-resized/resized_train/40891_right.jpeg', 0, 'diab']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27615\n",
       "2     6291\n",
       "1     2813\n",
       "3     1066\n",
       "4     1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use = df[df.src=='blnd'].copy()\n",
    "\n",
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 889, 3: 572, 4: 616, 1: 372}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randint(low=300, high=900):\n",
    "    res = np.random.randn()*300+600\n",
    "    return int(min(max(low, res), high))\n",
    "\n",
    "# set_torch_seed()\n",
    "n_t_extra = {2:get_randint(),3:get_randint(),4:get_randint(),1:get_randint()}\n",
    "n_t_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "for t,n in n_t_extra.items():\n",
    "    df_t_diab = df[(df.target==t) & (df.src=='diab')]\n",
    "    df2use = pd.concat([df2use, df_t_diab.sample(min(n, len(df_t_diab)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6111, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1888\n",
       "0    1805\n",
       "4     911\n",
       "3     765\n",
       "1     742\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: \n",
    "    df2use = df2use.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.62 s, sys: 102 ms, total: 5.72 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz=SZ, bs=BS):\n",
    "    src = (ImageList.from_df(df=df2use,path='./',cols='fnm') \n",
    "#             .split_by_rand_pct(0.2) \n",
    "            .split_none()\n",
    "            .label_from_df(cols='target',  \n",
    "                           #label_cls=FloatList\n",
    "                          )\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=sz,\n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "\n",
    "set_torch_seed()\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: test = test.head(dbgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test,\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME, num_classes=5) \n",
    "learn = Learner(data, model, path=p_o, \n",
    "#                 wd=WD,  \n",
    "#                 metrics=[accuracy, qwk],\n",
    "               )\n",
    "if FP16: learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "learn.recorder.plot(suggestion=True, skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.852415</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.873402</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.837801</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.806477</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.716041</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.629448</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.558470</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.566268</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.511560</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "learn.fit_one_cycle(10, max_lr=1e-3, \n",
    "#                     callbacks=[SaveModelCallback(learn, \n",
    "#                                                  every='epoch', \n",
    "#                                                  name=f'{PRFX}_model')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'rndmdl_seed_{SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd8VfX5wPHPk5BJApnskbBERgghgArIVBkqbqXa1slPXG0dLW5KS6Vqldo6qlZwggMHKu6CKCJ7b4QAYSWEESBASPL9/XFObu5N7s0guTk35Hm/Xnlx7rlnPPeE5Mn3fM/3+YoxBqWUUqqygpwOQCmlVN2iiUMppVSVaOJQSilVJZo4lFJKVYkmDqWUUlWiiUMppVSVaOJQSilVJZo4lFJKVYkmDqWUUlXSwOkAqiohIcEkJSU5HYZSStUpS5cu3W+MSayJY9W5xJGUlMSSJUucDkMppeoUEdleU8fSW1VKKaWqRBOHUkqpKtHEoZRSqkrqXB+HUurMcerUKTIzMzlx4oTToZwxwsPDadWqFSEhIX47hyYOpZRjMjMziY6OJikpCRFxOpw6zxhDTk4OmZmZJCcn++08eqtKKeWYEydOEB8fr0mjhogI8fHxfm/BaeJQSjlKk0bNqo3r6bfEISKviUiWiKwpZ5tBIrJCRNaKyPf+igUgv6CItxdu53h+oT9Po5RSZzx/tjimAcN9vSkiMcALwKXGmK7A1X6MhXmbsnn4ozWc/diX/jyNUqoOycnJITU1ldTUVJo1a0bLli1dr/Pz8yt1jJtuuomNGzf6OdLA4rfOcWPMPBFJKmeTXwEfGmN22Ntn+SsWgOAgbQ4rpTzFx8ezYsUKACZMmEBUVBT333+/xzbGGIwxBAV5/zt76tSpfo8z0DjZx9EJiBWRuSKyVER+48+TDe7cxLV8+Pgpf55KKVXHbdmyhW7dunH77beTlpbGnj17GDt2LOnp6XTt2pWJEye6tu3fvz8rVqygoKCAmJgYxo8fT48ePTj33HPJyvLr38OOcfJx3AZAL2AoEAEsEJGfjTGbSm8oImOBsQBt2rQ57RM+dVUKD3ywiuU7DjLorCYV76CUqjV//nQt63bn1ugxu7RoxOOXdD2tfdetW8fUqVN56aWXAJg8eTJxcXEUFBQwePBgrrrqKrp06eKxz+HDhxk4cCCTJ0/m3nvv5bXXXmP8+PHV/hyBxskWRybwpTHmmDFmPzAP6OFtQ2PMy8aYdGNMemLi6Rd3PLt5IwDuf3/VaR9DKVU/tG/fnt69e7teT58+nbS0NNLS0li/fj3r1q0rs09ERAQjRowAoFevXmRkZNRWuLXKyRbHJ8C/RaQBEAr0BZ715wm7trASx/6jJ/15GqXUaTjdloG/NGzY0LW8efNm/vnPf7Jo0SJiYmK44YYbvI6VCA0NdS0HBwdTUFBQK7HWNn8+jjsdWACcJSKZInKLiNwuIrcDGGPWA18Cq4BFwKvGGJ+P7tZQTESHW7lyX66WOFBKVU5ubi7R0dE0atSIPXv28NVXXzkdkqP8+VTVmEps8xTwlL9i8OaJK7pz1zvLueKFn5g/fkhtnlopVUelpaXRpUsXunXrRrt27ejXr5/TITlKjDFOx1Al6enppjoTORljSH5wNgAZk0fVVFhKqdOwfv16zj77bKfDOON4u64istQYk14Tx693JUfch+PnFxQ5GIlSStVN9S5xADx5ZQoAC7flOByJUkrVPfUycVzYtSkAv5uxwuFIlFKq7qmXiSMm0npk7sCxfOpaH49SSjmtXiYOgGvSWwFwME/LjyilVFXU28RRXHJk/Z6aLXGglFJnunqbONolWqNCM3KOORyJUsopgwYNKjOYb8qUKdxxxx0+94mKigJg9+7dXHXVVT6PW9GwgSlTppCXl+d6PXLkSA4dOlTZ0B1VbxNHpybRAMxevcfhSJRSThkzZgwzZszwWDdjxgzGjKlw/DItWrTggw8+OO1zl04cs2fPJiYm5rSPV5vqbeIIsufnmL9FH8lVqr666qqr+Oyzzzh50qpfl5GRwe7du0lNTWXo0KGkpaXRvXt3PvnkkzL7ZmRk0K1bNwCOHz/OddddR0pKCtdeey3Hjx93bTdu3DhXOfbHH38cgOeee47du3czePBgBg8eDEBSUhL79+8H4JlnnqFbt25069aNKVOmuM539tlnc9ttt9G1a1cuvPBCj/PUJieLHDpudGoLPlmxm5MFhYQ1CHY6HKXqty/Gw97VNXvMZt1hxGSfb8fHx9OnTx++/PJLRo8ezYwZM7j22muJiIjgo48+olGjRuzfv59zzjmHSy+91Od83i+++CKRkZGsWrWKVatWkZaW5npv0qRJxMXFUVhYyNChQ1m1ahX33HMPzzzzDHPmzCEhIcHjWEuXLmXq1KksXLgQYwx9+/Zl4MCBxMbGsnnzZqZPn84rr7zCNddcw8yZM7nhhhtq5lpVQb1tcQD062B9w/Ye1oKHStVX7rerim9TGWN46KGHSElJYdiwYezatYt9+/b5PMa8efNcv8BTUlJISUlxvffee++RlpZGz549Wbt2rddy7O5+/PFHLr/8cho2bEhUVBRXXHEFP/zwAwDJycmkpqYCzpZtr9ctjlYxEQDsOnSctvENK9haKeVX5bQM/Omyyy7j3nvvZdmyZRw/fpy0tDSmTZtGdnY2S5cuJSQkhKSkJK9l1N15a41s27aNp59+msWLFxMbG8uNN95Y4XHKG1sWFhbmWg4ODnbsVlW9bnE0axwOwB/e1RHkStVXUVFRDBo0iJtvvtnVKX748GGaNGlCSEgIc+bMYfv27eUe4/zzz+ftt98GYM2aNaxaZU0Wl5ubS8OGDWncuDH79u3jiy++cO0THR3NkSNHvB7r448/Ji8vj2PHjvHRRx8xYMCAmvq4NaJetzhax0UCEBGi/RtK1WdjxozhiiuucN2yuv7667nkkktIT08nNTWVzp07l7v/uHHjuOmmm0hJSSE1NZU+ffoA0KNHD3r27EnXrl3LlGMfO3YsI0aMoHnz5syZM8e1Pi0tjRtvvNF1jFtvvZWePXsG1GyC9a6semmjn59Po/AGvHlL3xo7plKqcrSsun9oWXU/a94onD3aOa6UUpWmiSMmnD2HjmuxQ6WUqqR6nzgAjuUXsnW/lh5Rygn6R1vNqo3rWe8TRwN7BPl3630/o62U8o/w8HBycnI0edQQYww5OTmEh4f79Tz1+qkqgN+el8QrP2yjUXiI06EoVe+0atWKzMxMsrOznQ7ljBEeHk6rVq38eo56nzgSoqwBNfuPnnQ4EqXqn5CQEJKTk50OQ1VRvb9VFR4STHR4A/YfzXc6FKWUqhPqfeIASIwOI/uItjiUUqoyNHEAiVFhZOutKqWUqhRNHFgtjv3a4lBKqUrxW+IQkddEJEtE1lSwXW8RKRQR73Mw1oLE6DCyNHEopVSl+LPFMQ0YXt4GIhIM/B34qrzt/C0hKoyjJws4carQyTCUUqpO8FviMMbMAw5UsNndwEwgy19xVEZCVCigj+QqpVRlONbHISItgcuBl5yKoVjJWA59JFcppSriZOf4FOBPxpgK7w+JyFgRWSIiS/wxwjQx2kocWblaJVcppSri5MjxdGCGPd1iAjBSRAqMMR+X3tAY8zLwMljzcdR0IM0bW1PIanl1pZSqmGOJwxjjqjMgItOAz7wljdoQ3zCU0OAgdh9yZv5epZSqS/yWOERkOjAISBCRTOBxIATAGON4v4a7oCCheUw4u7XFoZRSFfJb4jDGjKnCtjf6K47KatbImtBJKaVU+XTkuC2uYSgH8/SpKqWUqogmDltMZCiH8k45HYZSSgU8TRy2uIYhHDp+SmciU0qpCmjisMVGhlJYZMg9UeB0KEopFdA0cdhiIrXsiFJKVYYmDlvrWGsQ4DsLdzgciVJKBTZNHLbQBtal+O+P2xyORCmlApsmDlu3lo0B6Nws2uFIlFIqsGnisIUEB5EYHcaGvUecDkUppQKaJg43xeXVlVJK+aaJw83FKc0ByMvXR3KVUsoXTRxuWsZYT1ZplVyllPJNE4eb1nFW4th5QBOHUkr5oonDTevYSADGvb3U4UiUUipwaeJwU9w5fuJUkfZzKKWUD5o43AQFiWs5K1dLjyillDeaOEq5Z0gHAPbl6myASinljSaOUi7p0QKAfUe0xaGUUt5o4iilaeNwAPbp/ONKKeWVJo5SosOsadh/3prjcCRKKRWYNHGUImJ1kK/eddjhSJRSKjBp4vBieNdmRIYGOx2GUkoFJE0cXjRpFEZGTh75BUVOh6KUUgFHE4cXIcHWZdlxIM/hSJRSKvBo4vDi/E6JABzKy3c4EqWUCjyaOLyIjQwB4GDeKYcjUUqpwOO3xCEir4lIlois8fH+9SKyyv76SUR6+CuWqoqNDAXgoLY4lFKqDH+2OKYBw8t5fxsw0BiTAvwFeNmPsVRJbEMrcfzn+18cjkQppQJPA38d2BgzT0SSynn/J7eXPwOt/BVLVTW0H8X9JfuYw5EopVTgCZQ+jluAL5wOoljxIECAoiLjYCRKKRV4HE8cIjIYK3H8qZxtxorIEhFZkp2dXStxXZZqFTtctye3Vs6nlFJ1haOJQ0RSgFeB0cYYn8WhjDEvG2PSjTHpiYmJtRLbVb1aA3DkhE7opJRS7hxLHCLSBvgQ+LUxZpNTcfgSZ3eQHzimT1YppZQ7fz6OOx1YAJwlIpkicouI3C4it9ubPAbEAy+IyAoRWeKvWE5HbENrLMed7yxzOBKllAos/nyqakwF798K3Oqv81dX8VgOpZRSnvyWOOq68JBgWsZEEB+lCUQppdw5/lRVIOubHEfOUe3jUEopd5o4ytEqNoK9uSc4WVDodChKKRUwNHGUo32TKAqLDDtytLy6UkoV08RRjtZxkQBkHjzucCRKKRU4NHGUo1G49UjukZM6CFAppYpp4ihHdLj10NmREzovh1JKFdPEUY6oMCtxLN52wOFIlFIqcGjiKEekXV794xW7HY5EKaUChyaOcogI6W1jSYwOczoUpZQKGJo4KpDSKoY87RxXSikXTRwViIkM4Vh+IfkFRU6HopRSAaFSiUNE2otImL08SETuEZEY/4YWGGIjrUdy9+WecDgSpZQKDJVtccwECkWkA/BfIBl4x29RBZCIUOvJqgFPznE4EqWUCgyVTRxFxpgC4HJgijHmD0Bz/4UVOEIblFwiHc+hlFKVTxynRGQM8FvgM3tdiH9CCizxDUvKqu89rLerlFKqsonjJuBcYJIxZpuIJANv+S+swHFuu3h6J8UCcNsbATVJoVJKOaJSicMYs84Yc48xZrqIxALRxpjJfo4tIAQFCc9ckwpAhlbJVUqpSj9VNVdEGolIHLASmCoiz/g3tMDRMibC6RCUUipgVPZWVWNjTC5wBTDVGNMLGOa/sAJLUJC4lrP0sVylVD1X2cTRQESaA9dQ0jler4wf0RmAl+dtdTgSpZRyVmUTx0TgK+AXY8xiEWkHbPZfWIHn0h4tAHhn0Q6HI1FKKWc1qMxGxpj3gffdXm8FrvRXUIGoRUwEDUODGdm9XgxfUUopnyrbOd5KRD4SkSwR2SciM0Wklb+DCzTH8gt5f2mm02EopZSjKnuraiowC2gBtAQ+tdcppZSqZyqbOBKNMVONMQX21zQg0Y9xBaR7hnQA4MSpQocjUUop51Q2cewXkRtEJNj+ugHIKW8HEXnNvrW1xsf7IiLPicgWEVklImlVDb62NW0cDsDh41qzSilVf1U2cdyM9SjuXmAPcBVWGZLyTAOGl/P+CKCj/TUWeLGSsTgmJsKqW3XXO8scjkQppZxT2ZIjO4wxlxpjEo0xTYwxl2ENBixvn3nAgXI2GQ28YSw/AzH2WJGAlZzQEIDFGQcdjkQppZxTnRkA763muVsCO91eZ9rrAlbL2JLSI6cKdUZApVT9VJ3EIRVvUuX9jdcNRcaKyBIRWZKdnV3N056+RuElw160xLpSqr6qTuLw+ku+CjKB1m6vWwG7vZ7ImJeNMenGmPTEROce5hIR3rqlLwCZB487FodSSjmp3MQhIkdEJNfL1xGsMR3VMQv4jf101TnAYWPMnmoe0++Kb1eNeeVn5m7McjgapZSqfeWWHDHGRJ/ugUVkOjAISBCRTOBx7FkDjTEvAbOBkcAWII+Kn9IKCC1iwl3LN05dTMbkUQ5Go5RSta9StapOhzFmTAXvG+BOf53fX8IaBHNR16Z8tXaf06EopZQjqtPHUW/dd+FZruXlO/TRXKVU/aKJ4zS0T4xyLV/+wk8cPVngYDRKKVW7NHGchuAgYdpNvV2v1+467GA0SilVuzRxnKa+yfGu5YN5+Q5GopRStUsTx2mKCA12LW/JOupgJEopVbs0cVTD4oeHAbBtf57DkSilVO3RxFENidFhdG/ZmJnLdFZApVT9oYmjmnKOngQgafznbN53xOFolFLK/zRxVNPjl3Z1LT/8sdc5q5RS6oyiiaOahnRu4lpetK286UeUUurMoImjmkKCg/jmD+c7HYZSStUaTRw1oGPTaBKjwwBYuDWHNbsOc7KgkIz9xxyOTCmlap7fihzWNzf0bcuz327iiS82sGLnIVJaNWZV5mG+f2AQbeMbOh2eUkrVGG1x1JCr01sBsGLnIQBWZVplSDburXtPWhljSBr/OZ0e+cLpUJRSAUgTRw2Jaxjqdf1bC3fUciTVcygvn2/WWSXj8wuKOHhMy6kopTzpraoaEh4S7HX9vE3ZLN1+gF5t48rdv6jIEBRU3Wncy1dQWERwkCDi/TxPfrmBF+b+4rFu16HjxPpIikqp+klbHDUoY/IoPr+nf5n1V764oNz9fvPaIto9NNuj5tVfPlvHPdOXVyue137cRtL4z/kl+yjGGIb843uSH5zNkgzvjw1/sLTsCHhf2yql6i9NHDWsa4vGxESGADC8azPX+vJu+czblA3AsGe+Z0vWEVbsPMR/f9zGrJW7mbMhi8yDp1cLa+Jn6wAY+o/vOVlQxI4D1nGueqlsIssvKCLryEnX645NomgdF8H8X3JO69xKqTOX3qryg8UPD2PngTwSo8P4cu1eAJbtOMjQs5tWuO+wZ+Z5vL5p2mLX8md392d7Th6jUppXOaZl28vOVHjP9OUEBwnPXpvq6tcoNiqlOVO+3czOA8dZsfMQqa1jqnxOpdSZSVscfhASHES7xCiiw0NY+NBQAG55fYnXbRdU4S/6i//1I3e+s8z15BbA2t2HsaZvt/z507Ukjf+8zL5Tvt3s8Xr3oePMWrmbj5bvwhjDsXzPWQzfX1Jy2+qy5+dXOkal1JlPE4efJUaFuZa/slsf7sa88rPX/Z69tofPY1754k8ALM44wKjnfuS1+RmA1cE+1V5+8+ftHvsssvsq2iVaY0rGvlmSyG59fQnf2i2OBDvehOgwHr+ki2ubNTrLoVLKponDz9yflPq/N5dyoJy+jlv7J7uWz27eiBljzyGsQdlvUWGRYdr8ba5f5ou2Wa0W92TxqF1w8Vd923jsm3PUOv+aXbmudd9tyOJrO3Esfngo0287h4/vOI9z2pXMcnjxv34EYHXmYU6cKizvIyulznCaOGrBAxed5Vp+fNZa17L7LaZFDw/lWH7JL+R2CVGc0y6edROHc98Fnfjndakex5zw6Tr+/KnV+X3kREGZYxe7JKWFx+uJo7uW2cadiHBu+3hExNXJX+znrTlc8u8f6fzol+UWdCwoLCJp/Oc8+82mcs+llKqbNHHUgjsHd3Att7dvFd3x9lJue2MpAA+N7EyT6HCiwqyxIDf1SyLUbmkEBwl3D+3I6NSWZEwe5fX4uSdO+Tx32/hI5t4/yPXa/dYZeCa1Pwzr5PFefMMwQoNL/otc93LJbbVr/uP9EePCIsN9768E4J/fbfa6jVKqbtPEUUt+Gj8EgO83ZWOMYfbqvXy73ro9FN/Q+mV+be82RIYGc+N5ST6P0yQ6rMy6NbtySRr/OcXj+i5LLWllxDUMJSmhIZ/e1Z+R3ZuR1jbWY98R3UoeGf7dsI4e74U2CGLTpBGuKXJL23/0ZJl1czZk8cmK3a7Xr8zbyjX247+rMg/xyrytOuGVUnWcJo5a0iImgs7Nolm+4xDJD872eC8uyhqZ3aFJFOsmDi+3KOLE0V1p0Ticnx8cSmypW0nGWC2IiZd1c60rHtHevVVjXri+F+EhwXx770AAPr6zH8kJ1rn6dYjHF1/lVLw9KRZSqk9m0uz1LMo4wKZ9R7j03/OZNHs9Fzw7j50Ham6e9uU7DjJh1lr2Hj5RY8dUSvnm13EcIjIc+CcQDLxqjJlc6v02wOtAjL3NeGPM7DIHOkPsy/X+iy2hYdlWhC/DuzVneDdrHEdcw1AO5nneptqXe4JG4SHMHz+EoycKvB2CDk2iPG57+boFViy4VCmUIZ2b8L8NWa5pcwHGz1xFfkGRzzEmFz7rOT5lwJNzyJg8ir/NXs/W7KP89bLuNGscXm4cpW3Ym8v4matdjydP+ymD9/7vXN5ZuJ1/XJNaJm6lVM3wW4tDRIKB54ERQBdgjIh0KbXZI8B7xpiewHXAC/6KJxA8enHpj28pbnFU1W/tW1rug/M6NY0GoGVMBGc1iz6t43qz6OGhXJbagtZxEbz6m3SiwxuQefC4q4N/xuKdfLh8F/fb/RuV8dHyTF6et5Vv12dxzhPfVXq//IIibn19CcOn/OAxpgWsvpePV+xmm86FopTf+PNWVR9gizFmqzEmH5gBjC61jQEa2cuNgd2cwa5Ia0WX5tbH/eTOftw+sD0ATb30W1TGb85NYuvfRvLObX0Z2b0ZL93Qi+tLPX5bU5pEhzPlup788MchBAWJ60mumct2eTyeW9wCeuPmPl6P4/6k1h/erXyScbd+T66rf8iXYc987/HUmlKq5vgzcbQEdrq9zrTXuZsA3CAimcBs4G4/xhMQ3rmtL7Pu6keP1jGMH9GZjMmjaBB8+t+GoCAhMrQBL1zfi+HdmvmsfFvTGoVbdznvf38lX6zZU+b9AR0TvO73zDXeBzY2r8JtKm+3oF68Pq3MugVbtc6WUv7gz8Th7TdY6T8BxwDTjDGtgJHAmyJSJiYRGSsiS0RkSXZ2th9CrT0xkaGktKr7dZ9WTbjItVy65ZAxeZRHApv3wGBaxkTw1i19GdK5Kf8a09Nj++4tG7Pn8AlG//tHj/UFhUXMXJrJlqwj3Dh1keuxY/cWTuMIqwUzonvZvpWv1pQdqa+Uqj5/do5nAq3dXrei7K2oW4DhAMaYBSISDiQAWe4bGWNeBl4GSE9P1/sPASK1dYxHH8Oih4bSpFFJy2HyFd1ZmXmYNvGRzLcfRwYYenYTj+P8+py2/HHmKlbao9KLnwSbsXgnj9gj4AE+WJJJYnQY9763AoCZ484jtXUMpwqLAHj71r5c/+pC1/avL9jO6wus0fQ//HEwreMiXe8VFBYRJFLuHCjbc45x4lQRbeMjyT1xiibRVeu8V+pM5c8Wx2Kgo4gki0goVuf3rFLb7ACGAojI2UA4ULebFPXIBV1Kqv0O6JjgkTQAruvThieu6F5mv8jQBtx/Yclgw/M7JbqW1+/J5adf9rM68zDHTno+FbY95xh3T1/OqULrb4eGYcEEB4kr0fTrkMBnd/dnwYNDKO22NzwfHe7w8Bfc8faycvtBBj41l4umzKPzo1/SZ9J3rM7Uel1KgR9bHMaYAhG5C/gK61Hb14wxa0VkIrDEGDMLuA94RUT+gHUb60ajPZp1xu0D2/PUVxsB+PuVKVXa964hHUlPikOApo1KHg54Ye4vrhLvU671LLOyL9dzwGFcZNmn0bq1bOz1fBvsud/fXbzDlXi+XLuX5AdnM+2m3gw6y7MVVFRU9r/hj1v2072V9+MrVZ/4dRyHPSZjdql1j7ktrwP6+TMG5T/BQVLhGJDyuBdRLOY+L8gDH3j2nfy8zbOzu3QLx92c+wcx+Om5HutyT5ziTzNXl9n2pmmL2faE5+f42UvHeukWkFL1lY4cVwHhl7+NLLOuuGVQ7FCe75pcpSUnNGTbEyN5ZNTZNAy1bmWlTPja67bGWGXjr3zxJ1dy8NLgYKOWSlEK0MShAkRwkLDy8Qt9vh9aqpTJ+7efW+ExRYRbB7Sr1Ij0i//1I0u3H2TOxiyMMdzwX6uTvV1CSfmXjXs1cSgFmjhUAGkcEcLoVM8y8E9f3YOMyaM8Kvc+eWUKvZPiKn3cc9uXvSX2+s19WPzwsDITZn2/MZtst7nXx/QpGVC540DeaRVoPHz8VLlzzitV12jiUAHln9f19LhtdWWaNWb0hnNKfoFf07t1mf3K8/DILrzgNkAwY/IoBnZKJDE6jItLzVfy/tJMctx+yd86IJk3b+nD2PPbAXBBqZpbvhhjeOn7X7jvvZX0+PPX9PzLN1WKWalA5tfOcaVOR3CQ8Oy1Pdi496hrIGF0eAgzx51XpRHmxSJCgxnZvTnX9W7N2c0bebwXEhzEH4Z14tlvSyadWpVpjU15d+w5iAgDOibSv0MCL8/bCsDKnYfo0dr3IM7CIkP7h8rW6iwsMmzbf4yosAbl3j5b8EsOq3cd4tb+7codZ6KUUzRxqIB0ec9WZdb1KjWXSFVN9vHI8G/Pa8uijBzy8gtZvuOQ68mrCLtTHfAYCT/6+fnMuX+QqyT9sZMF5OUXkmjXHMvI8V5g0T2ZlB6QWOzoyQLXPPQH807xp+Gdq/IRlaoVeqtK1XsxkaG8fes59GztmZi6tvAcszFj7Dmu5d/PWA5YSaPr41/Re9K3rveKR7KXZ8CTc7yun7Foh2v5xbm/VBy8Ug7QxKGU7faB7VzLL/+6V5liiue0i3fV2Soerd718a9c71/7nwW8+sNWtmVbLY6k+LItimLhId5/9Eo/gqxUINJbVUrZmjQKr3BA4yU9WnD39OUs3HaA5+ds8Xhv4bYDLNx2wPX6P79OZ9zbS/n7lSm8sWA7n67czT+u7sH8X/bz4bJdGGPKVDNevesQoQ2CyC+wWi3etlHKaZo4lDpNxeVWfImPCuV/9w0CoHdSnKu18uHyTACWbD9Y5rHi2autir4dmkSxJesoG/cdoVOTaP7y+TqKigx/Ht0NpZymt6qUqgHqmxsLAAAWfElEQVRX92rFvRd08lhXXPK9tNsGWLfENpQaUPjC3JIWzGP2bJHzNmXT4eHZTJ2f4ar0q5TTNHEoVUXLHr3A43VkaDBPXd2De4Z25PKeJXOVhfiYoKtDkygA/vrZOnYeyHOtf/JLqwXTOCLENe3v32Zv8Ch/snnfEb5YvYdbpi22n+bS+lmq9kldK0abnp5ulixZUvGGSvmRMYbkB63Ha7+9d6ArGYD1SO3+IydJcitXUlrS+M9dy5/e1Z+9uSdcpd+XP3oBMZEhruOXp2VMhMdcJ0r5IiJLjTHpNXEs7eNQ6jSICDeel0SXFo08kgZAVFgDosIq/6N1SamZD2Mbli0X78uuQ8dZknGA9CqUYFGquvRWlVKnacKlXbkmvWrlTyry/QODXMtdWzTyvaGbq15awI6cklte0xftYMjTc8nY730golLVpYlDKQcUjwhPKTUxVNv4kttbb9/al8FnJTKwUyKrJ1xIG7eR5qNSPOdYP/+pOby5IANjDA9+uJqt+48x6Om5vP5TBvM2ZVNQWMT3myo/ueaHyzJJGv+5Rx+MUsW0j0MphxX3d7x5Sx8GdEz0ud2JU4V8uWYvEaHBXNilKasyDzP6+fke2/RNjvMYS1Lsvgs68Y9vrHpcWyaNoIGPjvt731vBh8t2uV4/NLIzY89vX+XPpAJPTfZxaItDqQCRFO+7Mx2s0eqX9WzJRV2bISL0aB3DxNFdPbbxljQAV9IAq5ZWUZFhwqy1/Pt/m13rjTEeScPaVlscqixNHEoFiMaR3sd9lOc35yaRMXkUfymVQO4a3MHnPsOemUe7h2Yz7acMnv56E8V3HeZt3l9m23cW7qhU7S1Vv2jiUMphb93Sl0t6tCC6Ck9ilebeNzL5iu7cf9FZ9EmK4/4LO5WzlyX5wdnkFxSx+9BxAFrFRrDtiZE0Crfi6fjwF64SKEqBJg6lHNe/YwL/GtOzWjWpzu9U0jdyrT3R1Xu3n8tdQzq61v+qb5sy+xXr9MgXPPihVU7+/dvPRUR4/JKSVszqXYdPOzZ15tHEodQZ4smrUrhnSIcyCejX57QFIDm+Id/eO9DjvSvcRroXa9bImmTqyl6teNcuJZ+Ve6LC8yeN/5zUiV+fVuyqbtEBgEqdIXyNKenfMYE3f95OSqvGdGgSxSd39qNZ43DCGgTROCKED5d7doi7J552idbgxnFvLyu3cnChXRflUN6p6n4MVQdoi0OpM9xFXZux6KGh9G0XD0CP1jE0bRROTGQoIsIHt5/r2nar23zvAPGlRrEv3JrDiH/+wKG8fIwxbNiby8a9R7j9raVez/3u4h0cPq7J5EyjLQ6l6oEmjXzPcd6rbSy3DUjmuj5tysxx7v569uo93PH2MgBSJ35DbGQIB720MJZuP0ivtrEsyTjAn2au5k8zV1c4z4mqW7TFoVQ9JyI8PKoL7ROjvL4fac+9Xpw0inlLGgBXvvgTRUWG/UfzXeuOnNBWx5nEr4lDRIaLyEYR2SIi431sc42IrBORtSLyjj/jUUpV3Zz7B1Vqu6Gdm7iW9x876XH76n8bsnQ8yBnEb4lDRIKB54ERQBdgjIh0KbVNR+BBoJ8xpivwe3/Fo5Q6PU0bhdOvQ7zrdWrrGK/b/e2K7q7lPpO+83jvdzNWkDLha3bk5LF2tz7aW9f5s8XRB9hijNlqjMkHZgCjS21zG/C8MeYggDEmy4/xKKVO09u3nsPMcedyfd82fDjuPB61ZygcndqCb+8dyLYnRtK0UbhHR3tpx08Vcv5Tcxj13I/kHD1J7olTnDhVWO55T5wq5M53lrFNK/0GFH92jrcEdrq9zgT6ltqmE4CIzAeCgQnGmC/9GJNS6jT1ahtHr7bWvB+/6tOGDXtyeeCiszw63kvPC/LpXf3LzDcC0Ouv37qWo8Ia0Dspltdu7F1mDErnR61fB4u3HWDRw8Nq7LOo6vFni8PbMNjSpXgbAB2BQcAY4FURKdMOFpGxIrJERJZkZ1e+NLRSyj8i7OlyvT2tVdwncn6nRLqXKhvvzdGTBczZmM3wKT+41r30/S8esyRmHTmJMYbv1u9j5tLM6n8AVS3+TByZgPuIpFbAbi/bfGKMOWWM2QZsxEokHowxLxtj0o0x6YmJvstOK6Wcl5zQkIzJo3jj5j4AbJ40gj8OP6vC/TbuO+JanvzFhjLvv7FgO7e8voT73l9JUZH/p4N4b/FOnY/EB38mjsVARxFJFpFQ4DpgVqltPgYGA4hIAtatq61+jEkpVctCgoO46bxk1+vLUlv43PbW1xfz5s/bvb73+Ky1ruUt2UdrLkAvThUW8ceZqxjw5Bw27M0lafznfLJiF0dPFvj1vHWFXydyEpGRwBSs/ovXjDGTRGQisMQYM0usG5r/AIYDhcAkY8yM8o6pEzkpVbftP3qSdLc+joiQYI776CRf8sgwYiNDaf/QbI/1Azom8OYtpbtMa87hvFP08FF367O7+9OtZcW34AJNnZnIyRgz2xjTyRjT3hgzyV73mDFmlr1sjDH3GmO6GGO6V5Q0lFJ1X0JUGG/d0peuLRqx6a8jWP+X4Yzq3rzMdr/q24aEqDCCg8p2l/6weT/+/KP3o+W++1HuemeZz/fqCx05rpSqdf07JvD5PQMIbWD9Cnr66h5ltnlo5NmuZW/zirj3idS0f3y9yed7GTl5rKnnZeY1cSilHBcRGszKxy7kmWt6MPf+Qfz50q5EuU1sdXP/ZK7v24blj15AUnwkAP/6bku1z/vC3C0s33GwzPoBnRKICAnm1v4lfTOrJlzoWr5x6qJqnxuscSru/SZ7Dh9n7+GKS9g7TYscKqUCQuPIEK5IawVAUoLn/OuRoQ2YdLk1Mv2DceeR/tdv+Xz1Hp4tKHK1WrxZknGAG6cuZskjwwgPCXat/92M5YQGB/G+/WjvtiesqsDPfbeFKd9t4qym0fRo3ZhHLu5CdHgIvZNjaRReMrWvex2uLVlH2Xv4BP07JlT5M4967gd+yS47uHHbEyOrNbGXv2niUErVKQlRYa7lz1fv5vKerXxue9VLC4CSgYR/vrQrl/VsyScrPEcGDHp6LpkHj7vmFdmwt+Q22O+GlYwQmD9+CP0m/8+j32XYM98D8NjFXZj42Tqm3tSbwWdZdbte+3EbRcbQo3UMK3Yc4rbz27n2y8sv8Jo0AL5cs5cRXvp9AoXeqlJK1Tnv/Z9V2iT3uHWbxxhDbqkKvCcLyj6p9fistQx+em6Z9dtz8lxJozwtYyK4e0gHjDGcOFXoMTPixM/WAXDT1MWs3HmI7TnHmPjZOv76+XqufmkBk2avZ5Nbv0x5/Sjj3l7G0u0HKozHKZo4lFJ1Tu+kWACmL9rB/qMnSX5wNikTvnbVvjpwLJ/v1nsvfXfgWL7X9aW9c6v3x31bxERQZKxWTJ+/fed1m5unLWbgU3PLrH9jQYZr+ftNVhWMDX8ZzqoJF/LklSke25YuYx9I9FaVUqrOKb7/v2HvEY8xIa/+sJWsIyd5Y0HJIML7LujEP74p+9f9woeGIlK2ku+6iRexed9RevioAuxr3hJ3OT6S01s/72DcoA7sPnScLVlHaRgaTHiI9XVN79Zc07s1O3LyOP+pOezLPcn+oyc9bs0FCm1xKKXqpOJWh7unv97kkTTAevQ3Y/IovrtvoMf6po3CaRIdzks3pPHYxSUzPkSGNvCZNADXU13FosMb8Nnd/XluTE+2/m1kmel2S+s3+X9cbfe93DG4Q5n328RH0rlZNAA3vLqw3GM5RROHUqpO+v2wsmM7vOluj/JunxjF5kkjAEhxK744vFtzbu6fzE/jh7ieripPXKnE8PthnejWsjGX9mhBUJC4Ss6DVXZ+86QRvHh9Gv/5da8yx/r1uW29nuN9uzz9hr1H+M/3v1QYU23TxKGUqpP6dUjghz8Odr3u2MT7LaQGwSW/5kKCg8iYPIpZd/Uvs12LmIhKPQLbIDiIF69Po38H6/HbwiLPmQ07N492LT99dQ9CgoMY0b05F3Vt5rFdn+Q4j0d83UW7rX/iiw0cPVnAve+u4JMVuyqMrzZoH4dSqs5qHRfJvAcG8836fdzSP5m9h0+wcFsOOUfzuaSH72KK1TWie3OGdWnKjMU7uTa9tcd7ZzUtSRwhwZ5/m//1sm488vEaAO4ZUqYQuIe59w9ikP0E2IRZa/lw+S7alLpN5hS/Fjn0By1yqJQKdHsPnyCsQRCxpW5rFRYZtmYfpaNbcinPe0t28scPVtEyJoJdh47zxBXdGdOnzWnFVGeKHCqlVH3UrHF4maQBEBwklU4aANekt6ZdQkN2HToOQPPGZSfOcoImDqWUCmDu87Kf177qZU38QROHUkoFsBvcnrwqry5XbQqMKJRSSnk1bmB7eraJ4Z3b/DdxVVXpU1VKKRXARISP7ujndBgetMWhlFKqSjRxKKWUqhJNHEoppapEE4dSSqkq0cShlFKqSjRxKKWUqhJNHEoppapEE4dSSqkqqXPVcUUkG9he4YbeJQD7azCcmqbxVY/GVz0aX/UEenxnGWMqX2GxHHVu5LgxJvF09xWRJTVVVtgfNL7q0fiqR+OrnroQX00dS29VKaWUqhJNHEoppaqkviWOl50OoAIaX/VofNWj8VVPvYmvznWOK6WUclZ9a3EopZSqpnqTOERkuIhsFJEtIjLewTgyRGS1iKwofspBROJE5BsR2Wz/G2uvFxF5zo55lYik+SGe10QkS0TWuK2rcjwi8lt7+80i8ls/xzdBRHbZ13CFiIx0e+9BO76NInKR2/oa//6LSGsRmSMi60VkrYj8zl4fENevnPgC5fqFi8giEVlpx/dne32yiCy0r8W7IhJqrw+zX2+x30+qKG4/xTdNRLa5Xb9Ue32t/3zYxw4WkeUi8pn92v/Xzxhzxn8BwcAvQDsgFFgJdHEolgwgodS6J4Hx9vJ44O/28kjgC0CAc4CFfojnfCANWHO68QBxwFb731h7OdaP8U0A7veybRf7exsGJNvf82B/ff+B5kCavRwNbLJjCIjrV058gXL9BIiyl0OAhfZ1eQ+4zl7/EjDOXr4DeMlevg54t7y4/RjfNOAqL9vX+s+Hffx7gXeAz+zXfr9+9aXF0QfYYozZaozJB2YAox2Oyd1o4HV7+XXgMrf1bxjLz0CMiDSvyRMbY+YBB6oZz0XAN8aYA8aYg8A3wHA/xufLaGCGMeakMWYbsAXre++X778xZo8xZpm9fARYD7QkQK5fOfH5UtvXzxhjjtovQ+wvAwwBPrDXl75+xdf1A2CoiEg5cfsrPl9q/edDRFoBo4BX7ddCLVy/+pI4WgI73V5nUv4PkD8Z4GsRWSoiY+11TY0xe8D6YQea2Oudiruq8TgR51327YDXim8FORmf3ezvifVXacBdv1LxQYBcP/s2ywogC+sX6i/AIWNMgZdzueKw3z8MxNdmfMaY4us3yb5+z4pIWOn4SsXhz+/vFOCPQJH9Op5auH71JXGIl3VOPU7WzxiTBowA7hSR88vZNpDiBt/x1HacLwLtgVRgD/APe70j8YlIFDAT+L0xJre8TX3EUdvxBcz1M8YUGmNSgVZYf+WeXc65HI9PRLoBDwKdgd5Yt5/+5ER8InIxkGWMWeq+upxz1Vh89SVxZAKt3V63AnY7EYgxZrf9bxbwEdYPy77iW1D2v1n25k7FXdV4ajVOY8w++we6CHiFkmZ1rccnIiFYv5TfNsZ8aK8OmOvnLb5Aun7FjDGHgLlYfQMxIlJcDsn9XK447PcbY93GrM34htu3AI0x5iQwFeeuXz/gUhHJwLp9OASrBeL/61dTHTSB/IVVk2srVsdPcedeVwfiaAhEuy3/hHWv8yk8O1OftJdH4dnZtshPcSXh2flcpXiw/urahtXxF2svx/kxvuZuy3/Auj8L0BXPTr6tWB27fvn+29fhDWBKqfUBcf3KiS9Qrl8iEGMvRwA/ABcD7+PZuXuHvXwnnp2775UXtx/ja+52facAk538+bDPMYiSznG/X78a/QUUyF9YTzxswrqH+rBDMbSzv0ErgbXFcWDdZ/wO2Gz/G+f2H/N5O+bVQLofYpqOdbviFNZfHrecTjzAzVidaluAm/wc35v2+VcBs/D8RfiwHd9GYIQ/v/9Af6wm/Spghf01MlCuXznxBcr1SwGW23GsAR5z+zlZZF+L94Ewe324/XqL/X67iuL2U3z/s6/fGuAtSp68qvWfD7fjD6Ikcfj9+unIcaWUUlVSX/o4lFJK1RBNHEoppapEE4dSSqkq0cShlFKqSjRxKKWUqhJNHCrgiEihXXV0pYgsE5HzKtg+RkTuqMRx54pIwM4J7QS70utVTseh6hZNHCoQHTfGpBpjemCVd3iigu1jsCp/BiS3UbxKnRE0cahA1wg4CFbNJRH5zm6FrBaR4gqtk4H2divlKXvbP9rbrBSRyW7Hu9qeY2GTiAywtw0WkadEZLFduO7/7PXNRWSefdw1xdu7E2t+lb/bx1wkIh3s9dNE5BkRmQP8Xaw5Oj62j/+ziKS4faapdqyrRORKe/2FIrLA/qzv2/WmEJHJIrLO3vZpe93VdnwrRWReBZ9JROTf9jE+p6QAo1KVpn8JqUAUYVckDceaU2KIvf4EcLkxJldEEoCfRWQWVlmPbsYqRoeIjMAqJd3XGJMnInFux25gjOkj1uRFjwPDsEajHzbG9LYrnc4Xka+BK4CvjDGTRCQYiPQRb659zN9glaC42F7fCRhmjCkUkX8By40xl4nIEKxSIKnAo/a5u9uxx9qf7RF732Mi8ifgXhH5N3A50NkYY0Qkxj7PY8BFxphdbut8faaewFlAd6ApsA54rVLfFaVsmjhUIDrulgTOBd6wq5IK8De7onARVunnpl72HwZMNcbkARhj3OfzKC5EuBSrBhbAhUCK273+xkBHYDHwml0o8GNjzAof8U53+/dZt/XvG2MK7eX+wJV2PP8TkXgRaWzHel3xDsaYg3bV0y5Yv+zBqg+1AMjFSp6v2q2Fz+zd5gPTROQ9t8/n6zOdD0y349otIv/z8ZmU8kkThwpoxpgF9l/giVj1khKBXsaYU3ZV0HAvuwm+y0KftP8tpOT/vwB3G2O+KnMgK0mNAt4UkaeMMW94C9PH8rFSMXnbz1usgjX3wxgv8fQBhmIlm7uAIcaY20Wkrx1n8VSmXj+T3dLSOkOqWrSPQwU0EemMVaE1B+uv5iw7aQwG2tqbHcGaGrXY18DNIhJpH8P9VpU3XwHj7JYFItJJRBqKSFv7fK8A/8Wawtaba93+XeBjm3nA9fbxBwH7jTU3xtdYCaD488YCPwP93PpLIu2YooDGxpjZwO+xbnUhIu2NMQuNMY8B+7FKZHv9THYc19l9IM2BwRVcG6XK0BaHCkTFfRxg/eX8W7uf4G3gUxFZglXpdQOAMSZHROaLyBrgC2PMA/Zf3UtEJB+YDTxUzvlexbpttUyse0PZWH0kg4AHROQUcBT4jY/9w0RkIdYfYmVaCbYJwFQRWQXkAb+11/8VeN6OvRD4szHmQxG5EZguJbPLPYKVID8RkXD7uvzBfu8pEelor/sOq/ryKh+f6SOsPqPVWNVuvy/nuijllVbHVaoa7Ntl6caY/U7HolRt0VtVSimlqkRbHEoppapEWxxKKaWqRBOHUkqpKtHEoZRSqko0cSillKoSTRxKKaWqRBOHUkqpKvl/fsI3q+yugMYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 4.57 s, total: 17.5 s\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds_tst = preds_tst.numpy().squeeze()\n",
    "preds_tst = np.argmax(preds_tst, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst_tta, _ = learn.TTA(ds_type=DatasetType.Test)\n",
    "preds_tst_tta = preds_tst_tta.numpy().squeeze()\n",
    "preds_tst_tta = np.argmax(preds_tst_tta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1277\n",
       "0     353\n",
       "1     124\n",
       "4      96\n",
       "3      78\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds_tst.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.Series(preds_tst_tta.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          4\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "subm['diagnosis'] = preds_tst\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1277\n",
       "0     353\n",
       "1     124\n",
       "4      96\n",
       "3      78\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
