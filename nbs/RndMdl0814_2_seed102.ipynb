{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = f'RndMdl0814_2_seed{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'\n",
    "\n",
    "# p_o = f'.'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = False\n",
    "if dbg: dbgsz=500\n",
    "\n",
    "from fastai.vision import * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "source": [
    "!pip install ../input/efficientnetpytorch/efficientnet_pytorch-0.3.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 15 02:12:01 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    42W / 300W |     10MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading: \"http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\" to /tmp/.cache/torch/checkpoints/efficientnet-b3-c8376fa2.pth\n",
    "import os\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "\n",
    "!cp ../input/efficientnetpytorch/*.pth /tmp/.cache/torch/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 456\n"
     ]
    }
   ],
   "source": [
    "BS = 16\n",
    "FP16 = True\n",
    "PERC_VAL = 0.1\n",
    "WD = 0.01\n",
    "\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_open_yz = True\n",
    "\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "def load_ben_color(fn)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = crop_image_from_gray(image)\n",
    "    image, _ = crop_margin(image)\n",
    "    image = center_crop(image)\n",
    "    image = cv2.resize(image, (640, 480))#most common in test\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) , -4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfms = dict(\n",
    "    do_flip=True,\n",
    "    flip_vert=True,\n",
    "    max_rotate=360,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "    \n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "    \n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = load_ben_color(fn)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "    \n",
    "if use_open_yz:\n",
    "    vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "#         quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        quadratic_weighted_kappa(np.argmax(y_pred,1), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    aug_tfms = [o for o in learn.data.train_ds.tfms if o.tfm !=zoom]\n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "t2c_trn_has = dict(cnt)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/diabetic-retinopathy-resized'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'trainLabels.csv')\n",
    "img2grd_diab = [(f'{p}/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "# img2grd_diab = [(f'{p}/resized_train/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38788, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2791, '../input/aptos2019-blindness-detection/train_images/c0e15e8e2b46.png', 1, 'blnd'],\n",
       "       [31352, '../input/diabetic-retinopathy-resized/resized_train/35054_left.jpeg', 1, 'diab'],\n",
       "       [33591, '../input/diabetic-retinopathy-resized/resized_train/37846_right.jpeg', 2, 'diab'],\n",
       "       [25450, '../input/diabetic-retinopathy-resized/resized_train/27458_left.jpeg', 0, 'diab'],\n",
       "       [6982, '../input/diabetic-retinopathy-resized/resized_train/4148_left.jpeg', 2, 'diab']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27615\n",
       "2     6291\n",
       "1     2813\n",
       "3     1066\n",
       "4     1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use = df[df.src=='blnd'].copy()\n",
    "\n",
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 478, 3: 900, 4: 524, 1: 300}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randint(low=300, high=900):\n",
    "    res = np.random.randn()*300+600\n",
    "    return int(min(max(low, res), high))\n",
    "\n",
    "# set_torch_seed()\n",
    "n_t_extra = {2:get_randint(),3:get_randint(),4:get_randint(),1:get_randint()}\n",
    "n_t_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "for t,n in n_t_extra.items():\n",
    "    df_t_diab = df[(df.target==t) & (df.src=='diab')]\n",
    "    df2use = pd.concat([df2use, df_t_diab.sample(min(n, len(df_t_diab)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5837, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2    1477\n",
       "3    1066\n",
       "4     819\n",
       "1     670\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: \n",
    "    df2use = df2use.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.41 s, sys: 278 ms, total: 5.69 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz=SZ, bs=BS):\n",
    "    src = (ImageList.from_df(df=df2use,path='./',cols='fnm') \n",
    "#             .split_by_rand_pct(0.2) \n",
    "            .split_none()\n",
    "            .label_from_df(cols='target',  \n",
    "                           #label_cls=FloatList\n",
    "                          )\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=sz,\n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "\n",
    "set_torch_seed()\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: test = test.head(dbgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test,\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME, num_classes=5) \n",
    "learn = Learner(data, model, path=p_o, \n",
    "#                 wd=WD,  \n",
    "#                 metrics=[accuracy, qwk],\n",
    "               )\n",
    "if FP16: learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "learn.recorder.plot(suggestion=True, skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.846001</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.832019</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.807900</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.769035</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.724488</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.642198</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.618227</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.572432</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.525861</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.501704</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "learn.fit_one_cycle(10, max_lr=1e-3, \n",
    "#                     callbacks=[SaveModelCallback(learn, \n",
    "#                                                  every='epoch', \n",
    "#                                                  name=f'{PRFX}_model')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'rndmdl_seed_{SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 4.49 s, total: 17.1 s\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds_tst = preds_tst.numpy().squeeze()\n",
    "preds_tst = np.argmax(preds_tst, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst_tta, _ = learn.TTA(ds_type=DatasetType.Test)\n",
    "preds_tst_tta = preds_tst_tta.numpy().squeeze()\n",
    "preds_tst_tta = np.argmax(preds_tst_tta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1212\n",
       "0     342\n",
       "1     162\n",
       "4     121\n",
       "3      91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds_tst.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.Series(preds_tst_tta.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          4\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "subm['diagnosis'] = preds_tst\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1212\n",
       "0     342\n",
       "1     162\n",
       "4     121\n",
       "3      91\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
