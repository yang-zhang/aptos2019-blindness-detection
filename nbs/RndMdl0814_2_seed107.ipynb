{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = f'RndMdl0814_2_seed{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'\n",
    "\n",
    "# p_o = f'.'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = False\n",
    "if dbg: dbgsz=500\n",
    "\n",
    "from fastai.vision import * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "source": [
    "!pip install ../input/efficientnetpytorch/efficientnet_pytorch-0.3.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 15 12:20:05 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   46C    P0    42W / 300W |     10MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading: \"http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\" to /tmp/.cache/torch/checkpoints/efficientnet-b3-c8376fa2.pth\n",
    "import os\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "\n",
    "!cp ../input/efficientnetpytorch/*.pth /tmp/.cache/torch/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 456\n"
     ]
    }
   ],
   "source": [
    "BS = 16\n",
    "FP16 = True\n",
    "PERC_VAL = 0.1\n",
    "WD = 0.01\n",
    "\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_open_yz = True\n",
    "\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "def load_ben_color(fn)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = crop_image_from_gray(image)\n",
    "    image, _ = crop_margin(image)\n",
    "    image = center_crop(image)\n",
    "    image = cv2.resize(image, (640, 480))#most common in test\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) , -4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfms = dict(\n",
    "    do_flip=True,\n",
    "    flip_vert=True,\n",
    "    max_rotate=360,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "    \n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "    \n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = load_ben_color(fn)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "    \n",
    "if use_open_yz:\n",
    "    vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "#         quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        quadratic_weighted_kappa(np.argmax(y_pred,1), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    aug_tfms = [o for o in learn.data.train_ds.tfms if o.tfm !=zoom]\n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "t2c_trn_has = dict(cnt)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/diabetic-retinopathy-resized'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'trainLabels.csv')\n",
    "img2grd_diab = [(f'{p}/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "# img2grd_diab = [(f'{p}/resized_train/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38788, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14549, '../input/diabetic-retinopathy-resized/resized_train/13688_right.jpeg', 0, 'diab'],\n",
       "       [8132, '../input/diabetic-retinopathy-resized/resized_train/5632_left.jpeg', 2, 'diab'],\n",
       "       [8024, '../input/diabetic-retinopathy-resized/resized_train/5496_left.jpeg', 2, 'diab'],\n",
       "       [22516, '../input/diabetic-retinopathy-resized/resized_train/23786_left.jpeg', 1, 'diab'],\n",
       "       [11344, '../input/diabetic-retinopathy-resized/resized_train/9670_left.jpeg', 1, 'diab']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27615\n",
       "2     6291\n",
       "1     2813\n",
       "3     1066\n",
       "4     1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use = df[df.src=='blnd'].copy()\n",
    "\n",
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 577, 3: 900, 4: 300, 1: 325}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randint(low=300, high=900):\n",
    "    res = np.random.randn()*300+600\n",
    "    return int(min(max(low, res), high))\n",
    "\n",
    "# set_torch_seed()\n",
    "n_t_extra = {2:get_randint(),3:get_randint(),4:get_randint(),1:get_randint()}\n",
    "n_t_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "for t,n in n_t_extra.items():\n",
    "    df_t_diab = df[(df.target==t) & (df.src=='diab')]\n",
    "    df2use = pd.concat([df2use, df_t_diab.sample(min(n, len(df_t_diab)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5737, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2    1576\n",
       "3    1066\n",
       "1     695\n",
       "4     595\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: \n",
    "    df2use = df2use.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.19 s, sys: 218 ms, total: 6.41 s\n",
      "Wall time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz=SZ, bs=BS):\n",
    "    src = (ImageList.from_df(df=df2use,path='./',cols='fnm') \n",
    "#             .split_by_rand_pct(0.2) \n",
    "            .split_none()\n",
    "            .label_from_df(cols='target',  \n",
    "                           #label_cls=FloatList\n",
    "                          )\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=sz,\n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "\n",
    "set_torch_seed()\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: test = test.head(dbgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test,\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME, num_classes=5) \n",
    "learn = Learner(data, model, path=p_o, \n",
    "#                 wd=WD,  \n",
    "#                 metrics=[accuracy, qwk],\n",
    "               )\n",
    "if FP16: learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "learn.recorder.plot(suggestion=True, skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.814248</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.829080</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.800564</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.755583</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.721504</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681959</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.598609</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.569528</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.520974</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "learn.fit_one_cycle(10, max_lr=1e-3, \n",
    "#                     callbacks=[SaveModelCallback(learn, \n",
    "#                                                  every='epoch', \n",
    "#                                                  name=f'{PRFX}_model')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'rndmdl_seed_{SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 4.35 s, total: 17.1 s\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds_tst = preds_tst.numpy().squeeze()\n",
    "preds_tst = np.argmax(preds_tst, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst_tta, _ = learn.TTA(ds_type=DatasetType.Test)\n",
    "preds_tst_tta = preds_tst_tta.numpy().squeeze()\n",
    "preds_tst_tta = np.argmax(preds_tst_tta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1230\n",
       "0     332\n",
       "1     158\n",
       "3     107\n",
       "4     101\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds_tst.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.Series(preds_tst_tta.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          4\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "subm['diagnosis'] = preds_tst\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1230\n",
       "0     332\n",
       "1     158\n",
       "3     107\n",
       "4     101\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
