{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = f'RndMdl0814_2_seed{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'\n",
    "\n",
    "# p_o = f'.'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = False\n",
    "if dbg: dbgsz=500\n",
    "\n",
    "from fastai.vision import * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "source": [
    "!pip install ../input/efficientnetpytorch/efficientnet_pytorch-0.3.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 15 03:00:44 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    41W / 300W |     10MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading: \"http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\" to /tmp/.cache/torch/checkpoints/efficientnet-b3-c8376fa2.pth\n",
    "import os\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "\n",
    "!cp ../input/efficientnetpytorch/*.pth /tmp/.cache/torch/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 456\n"
     ]
    }
   ],
   "source": [
    "BS = 16\n",
    "FP16 = True\n",
    "PERC_VAL = 0.1\n",
    "WD = 0.01\n",
    "\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_open_yz = True\n",
    "\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "def load_ben_color(fn)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = crop_image_from_gray(image)\n",
    "    image, _ = crop_margin(image)\n",
    "    image = center_crop(image)\n",
    "    image = cv2.resize(image, (640, 480))#most common in test\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) , -4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfms = dict(\n",
    "    do_flip=True,\n",
    "    flip_vert=True,\n",
    "    max_rotate=360,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "    \n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "    \n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = load_ben_color(fn)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "    \n",
    "if use_open_yz:\n",
    "    vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "#         quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        quadratic_weighted_kappa(np.argmax(y_pred,1), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    aug_tfms = [o for o in learn.data.train_ds.tfms if o.tfm !=zoom]\n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "t2c_trn_has = dict(cnt)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/diabetic-retinopathy-resized'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'trainLabels.csv')\n",
    "img2grd_diab = [(f'{p}/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "# img2grd_diab = [(f'{p}/resized_train/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38788, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27829, '../input/diabetic-retinopathy-resized/resized_train/30550_right.jpeg', 1, 'diab'],\n",
       "       [27537, '../input/diabetic-retinopathy-resized/resized_train/30144_right.jpeg', 2, 'diab'],\n",
       "       [36878, '../input/diabetic-retinopathy-resized/resized_train/41940_left.jpeg', 1, 'diab'],\n",
       "       [19974, '../input/diabetic-retinopathy-resized/resized_train/20442_left.jpeg', 0, 'diab'],\n",
       "       [36036, '../input/diabetic-retinopathy-resized/resized_train/40877_left.jpeg', 0, 'diab']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27615\n",
       "2     6291\n",
       "1     2813\n",
       "3     1066\n",
       "4     1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use = df[df.src=='blnd'].copy()\n",
    "\n",
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 900, 3: 334, 4: 749, 1: 463}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randint(low=300, high=900):\n",
    "    res = np.random.randn()*300+600\n",
    "    return int(min(max(low, res), high))\n",
    "\n",
    "# set_torch_seed()\n",
    "n_t_extra = {2:get_randint(),3:get_randint(),4:get_randint(),1:get_randint()}\n",
    "n_t_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "for t,n in n_t_extra.items():\n",
    "    df_t_diab = df[(df.target==t) & (df.src=='diab')]\n",
    "    df2use = pd.concat([df2use, df_t_diab.sample(min(n, len(df_t_diab)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6067, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1899\n",
       "0    1805\n",
       "4    1003\n",
       "1     833\n",
       "3     527\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: \n",
    "    df2use = df2use.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.54 s, sys: 323 ms, total: 5.86 s\n",
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz=SZ, bs=BS):\n",
    "    src = (ImageList.from_df(df=df2use,path='./',cols='fnm') \n",
    "#             .split_by_rand_pct(0.2) \n",
    "            .split_none()\n",
    "            .label_from_df(cols='target',  \n",
    "                           #label_cls=FloatList\n",
    "                          )\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=sz,\n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "\n",
    "set_torch_seed()\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: test = test.head(dbgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test,\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME, num_classes=5) \n",
    "learn = Learner(data, model, path=p_o, \n",
    "#                 wd=WD,  \n",
    "#                 metrics=[accuracy, qwk],\n",
    "               )\n",
    "if FP16: learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "learn.recorder.plot(suggestion=True, skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.801774</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.793540</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.814738</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.777992</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.705666</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.649070</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.605865</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.561073</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.493951</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.500305</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "learn.fit_one_cycle(10, max_lr=1e-3, \n",
    "#                     callbacks=[SaveModelCallback(learn, \n",
    "#                                                  every='epoch', \n",
    "#                                                  name=f'{PRFX}_model')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'rndmdl_seed_{SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 4.46 s, total: 17.1 s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds_tst = preds_tst.numpy().squeeze()\n",
    "preds_tst = np.argmax(preds_tst, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst_tta, _ = learn.TTA(ds_type=DatasetType.Test)\n",
    "preds_tst_tta = preds_tst_tta.numpy().squeeze()\n",
    "preds_tst_tta = np.argmax(preds_tst_tta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1277\n",
       "0     362\n",
       "1     122\n",
       "4      88\n",
       "3      79\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds_tst.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.Series(preds_tst_tta.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          3\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "subm['diagnosis'] = preds_tst\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1277\n",
       "0     362\n",
       "1     122\n",
       "4      88\n",
       "3      79\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
