{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = f'RndMdl0814_2_seed{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'\n",
    "\n",
    "# p_o = f'.'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = False\n",
    "if dbg: dbgsz=500\n",
    "\n",
    "from fastai.vision import * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "source": [
    "!pip install ../input/efficientnetpytorch/efficientnet_pytorch-0.3.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 15 03:43:28 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   48C    P0    43W / 300W |     10MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading: \"http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\" to /tmp/.cache/torch/checkpoints/efficientnet-b3-c8376fa2.pth\n",
    "import os\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "\n",
    "!cp ../input/efficientnetpytorch/*.pth /tmp/.cache/torch/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 456\n"
     ]
    }
   ],
   "source": [
    "BS = 16\n",
    "FP16 = True\n",
    "PERC_VAL = 0.1\n",
    "WD = 0.01\n",
    "\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_open_yz = True\n",
    "\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "def load_ben_color(fn)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = crop_image_from_gray(image)\n",
    "    image, _ = crop_margin(image)\n",
    "    image = center_crop(image)\n",
    "    image = cv2.resize(image, (640, 480))#most common in test\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) , -4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfms = dict(\n",
    "    do_flip=True,\n",
    "    flip_vert=True,\n",
    "    max_rotate=360,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "    \n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "    \n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = load_ben_color(fn)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "    \n",
    "if use_open_yz:\n",
    "    vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "#         quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        quadratic_weighted_kappa(np.argmax(y_pred,1), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    aug_tfms = [o for o in learn.data.train_ds.tfms if o.tfm !=zoom]\n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "t2c_trn_has = dict(cnt)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/diabetic-retinopathy-resized'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'trainLabels.csv')\n",
    "img2grd_diab = [(f'{p}/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "# img2grd_diab = [(f'{p}/resized_train/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38788, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26616, '../input/diabetic-retinopathy-resized/resized_train/29002_left.jpeg', 0, 'diab'],\n",
       "       [11098, '../input/diabetic-retinopathy-resized/resized_train/9348_left.jpeg', 0, 'diab'],\n",
       "       [26232, '../input/diabetic-retinopathy-resized/resized_train/28468_left.jpeg', 0, 'diab'],\n",
       "       [27238, '../input/diabetic-retinopathy-resized/resized_train/29782_left.jpeg', 0, 'diab'],\n",
       "       [28216, '../input/diabetic-retinopathy-resized/resized_train/31035_left.jpeg', 1, 'diab']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27615\n",
       "2     6291\n",
       "1     2813\n",
       "3     1066\n",
       "4     1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use = df[df.src=='blnd'].copy()\n",
    "\n",
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 703, 3: 300, 4: 300, 1: 300}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randint(low=300, high=900):\n",
    "    res = np.random.randn()*300+600\n",
    "    return int(min(max(low, res), high))\n",
    "\n",
    "# set_torch_seed()\n",
    "n_t_extra = {2:get_randint(),3:get_randint(),4:get_randint(),1:get_randint()}\n",
    "n_t_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "for t,n in n_t_extra.items():\n",
    "    df_t_diab = df[(df.target==t) & (df.src=='diab')]\n",
    "    df2use = pd.concat([df2use, df_t_diab.sample(min(n, len(df_t_diab)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5265, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2    1702\n",
       "1     670\n",
       "4     595\n",
       "3     493\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: \n",
    "    df2use = df2use.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.87 s, sys: 220 ms, total: 5.09 s\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz=SZ, bs=BS):\n",
    "    src = (ImageList.from_df(df=df2use,path='./',cols='fnm') \n",
    "#             .split_by_rand_pct(0.2) \n",
    "            .split_none()\n",
    "            .label_from_df(cols='target',  \n",
    "                           #label_cls=FloatList\n",
    "                          )\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=sz,\n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "\n",
    "set_torch_seed()\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: test = test.head(dbgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test,\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME, num_classes=5) \n",
    "learn = Learner(data, model, path=p_o, \n",
    "#                 wd=WD,  \n",
    "#                 metrics=[accuracy, qwk],\n",
    "               )\n",
    "if FP16: learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "learn.recorder.plot(suggestion=True, skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.844348</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.815414</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.814752</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.726697</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.690091</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.635542</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.534533</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.512349</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "learn.fit_one_cycle(10, max_lr=1e-3, \n",
    "#                     callbacks=[SaveModelCallback(learn, \n",
    "#                                                  every='epoch', \n",
    "#                                                  name=f'{PRFX}_model')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'rndmdl_seed_{SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 s, sys: 4.27 s, total: 17 s\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds_tst = preds_tst.numpy().squeeze()\n",
    "preds_tst = np.argmax(preds_tst, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst_tta, _ = learn.TTA(ds_type=DatasetType.Test)\n",
    "preds_tst_tta = preds_tst_tta.numpy().squeeze()\n",
    "preds_tst_tta = np.argmax(preds_tst_tta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1202\n",
       "0     363\n",
       "1     152\n",
       "4     112\n",
       "3      99\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds_tst.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.Series(preds_tst_tta.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          4\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "subm['diagnosis'] = preds_tst\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1202\n",
       "0     363\n",
       "1     152\n",
       "4     112\n",
       "3      99\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
