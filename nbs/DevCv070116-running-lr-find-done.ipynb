{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/tanlikesmath/intro-aptos-diabetic-retinopathy-eda-starter\n",
    "- https://medium.com/@btahir/a-quick-guide-to-using-regression-with-image-data-in-fastai-117304c0af90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.54'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbg = True\n",
    "if dbg:\n",
    "    dbgsz = 30000\n",
    "\n",
    "PRFX = 'DevCv070116'\n",
    "SEED = 111\n",
    "\n",
    "import fastai\n",
    "fastai.__version__\n",
    "\n",
    "# !pip install ../input/fastai1054/fastai-1.0.54-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True \n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "p_o = f'../output/{PRFX}'\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2grd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1])  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38788"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 27615), (2, 6291), (1, 2813), (3, 1066), (4, 1003)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "92364"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 67148), (2, 14152), (1, 6575), (3, 2280), (4, 2209)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = '../input/diabetic-retinopathy-detection'\n",
    "pp = Path(p)\n",
    "train=pd.read_csv(pp/'trainLabels.csv')\n",
    "test=pd.read_csv(pp/'retinopathy_solution.csv')\n",
    "\n",
    "img2grd_diab_train=[(f'{p}/train_images/{o[0]}.jpeg',o[1])  for o in train.values]\n",
    "img2grd_diab_test=[(f'{p}/test_images/{o[0]}.jpeg',o[1])  for o in test.values]\n",
    "img2grd += img2grd_diab_train\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())\n",
    "img2grd += img2grd_diab_test\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92777"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 67282), (2, 14288), (1, 6595), (3, 2354), (4, 2258)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 67316), (2, 14320), (1, 6600), (3, 2373), (4, 2271)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = '../input/IDRID/B. Disease Grading'\n",
    "pp = Path(p)\n",
    "train=pd.read_csv(pp/'2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv')\n",
    "test=pd.read_csv(pp/'2. Groundtruths/b. IDRiD_Disease Grading_Testing Labels.csv')\n",
    "\n",
    "img2grd_idrid_train=[(f'{p}/1. Original Images/a. Training Set/{o[0]}.jpg',o[1])  for o in train.values]\n",
    "img2grd_idrid_test=[(f'{p}/1. Original Images/b. Testing Set/{o[0]}.jpg',o[1])  for o in test.values]\n",
    "img2grd += img2grd_idrid_train\n",
    "display(len(img2grd))\n",
    "display(Counter(o[1] for o in img2grd).most_common())\n",
    "img2grd += img2grd_idrid_test\n",
    "len(img2grd)\n",
    "display(Counter(o[1] for o in img2grd).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2grd = np.array(img2grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are here!\n"
     ]
    }
   ],
   "source": [
    "if np.all([Path(o[0]).exists() for o in img2grd]): print('All files are here!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fnm</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/aptos2019-blindness-detection/train_i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 fnm target\n",
       "0  ../input/aptos2019-blindness-detection/train_i...      2\n",
       "1  ../input/aptos2019-blindness-detection/train_i...      4\n",
       "2  ../input/aptos2019-blindness-detection/train_i...      1\n",
       "3  ../input/aptos2019-blindness-detection/train_i...      0\n",
       "4  ../input/aptos2019-blindness-detection/train_i...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "idx_blnd_train = np.where(df.fnm.str.contains('aptos2019-blindness-detection/train_images'))[0]\n",
    "idx_val = np.random.choice(idx_blnd_train, len_blnd_test, replace=False)\n",
    "df['is_val']=False\n",
    "df.loc[idx_val, 'is_val']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg:\n",
    "    df=df.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=True,flip_vert=True,\n",
    "                      max_rotate=360,max_warp=0,\n",
    "                      max_zoom=1.1,max_lighting=0.1,p_lighting=0.5)\n",
    "\n",
    "def get_data(sz, bs):\n",
    "    src = (ImageList.from_df(df=df,path='./',cols='fnm') #get dataset from dataset\n",
    "            .split_from_df(col='is_val') #Splitting the dataset\n",
    "            .label_from_df(cols='target',  label_cls=FloatList) #obtain labels from the level column\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms,size=sz,\n",
    "                         resize_method=ResizeMethod.SQUISH,\n",
    "                         padding_mode='zeros') #Data augmentation\n",
    "            .databunch(bs=bs,num_workers=0) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128 #smaller batch size is better for training, but may take longer\n",
    "sz = 224\n",
    "data = get_data(sz, bs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data.show_batch(rows=3, figsize=(7,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, base_arch=models.resnet50, \n",
    "                    metrics = [mse], path=p_o)\n",
    "learn.loss = MSELossFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.91E-02\n",
      "Min loss divided by 10: 1.20E-02\n",
      "CPU times: user 54min 17s, sys: 21min 2s, total: 1h 15min 19s\n",
      "Wall time: 55min 36s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "learn.freeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='4', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [2/4 5:06:36<5:06:36]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.803725</td>\n",
       "      <td>0.609977</td>\n",
       "      <td>0.609976</td>\n",
       "      <td>2:32:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.608607</td>\n",
       "      <td>0.396634</td>\n",
       "      <td>0.396634</td>\n",
       "      <td>2:34:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='146' class='' max='219', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      66.67% [146/219 1:39:23<49:41 0.5608]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4,max_lr = 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(6, max_lr=slice(1e-6,1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds_val, y_val = learn.get_preds(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val = preds_val.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val= y_val.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{p_o}/preds_val.npy', preds_val)\n",
    "np.save(f'{p_o}/y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/88773#latest-515044\n",
    "# We used OptimizedRounder given by hocop1. https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107#480970\n",
    "# put numerical value to one of bins\n",
    "def to_bins(x, borders):\n",
    "    for i in range(len(borders)):\n",
    "        if x <= borders[i]:\n",
    "            return i\n",
    "    return len(borders)\n",
    "\n",
    "class Hocop1OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _loss(self, coef, X, y, idx):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        ll = -quadratic_weighted_kappa(y, X_p)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        coef = [1.5, 2.0, 2.5, 3.0]\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n",
    "        for it1 in range(10):\n",
    "            for idx in range(4):\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                coef[idx] = a\n",
    "                la = self._loss(coef, X, y, idx)\n",
    "                coef[idx] = b\n",
    "                lb = self._loss(coef, X, y, idx)\n",
    "                for it in range(20):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        coef[idx] = a\n",
    "                        la = self._loss(coef, X, y, idx)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        coef[idx] = b\n",
    "                        lb = self._loss(coef, X, y, idx)\n",
    "        self.coef_ = {'x': coef}\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107#480970\n",
    "class AbhishekOptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket(preds_raw, coef = [0.5, 1.5, 2.5, 3.5]):\n",
    "    preds = np.zeros(preds_raw.shape)\n",
    "    for i, pred in enumerate(preds_raw):\n",
    "        if pred < coef[0]:\n",
    "            preds[i] = 0\n",
    "        elif pred >= coef[0] and pred < coef[1]:\n",
    "            preds[i] = 1\n",
    "        elif pred >= coef[1] and pred < coef[2]:\n",
    "            preds[i] = 2\n",
    "        elif pred >= coef[2] and pred < coef[3]:\n",
    "            preds[i] = 3\n",
    "        else:\n",
    "            preds[i] = 4\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2coefs = {'simple': [0.5, 1.5, 2.5, 3.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "optR = Hocop1OptimizedRounder()\n",
    "optR.fit(preds_val, y_val)\n",
    "optnm2coefs['hocop1'] = optR.coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "optR = AbhishekOptimizedRounder()\n",
    "optR.fit(preds_val, y_val)\n",
    "optnm2coefs['abhishek'] = optR.coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2preds_val_grd = {k: bucket(preds_val, coef) for k,coef in optnm2coefs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2qwk = {k: quadratic_weighted_kappa(y_val, preds) for k,preds in optnm2preds_val_grd.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(y_val).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2preds_val_grd['abhishek'].squeeze().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(optnm2preds_val_grd['abhishek'].squeeze()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(optnm2preds_val_grd['abhishek'], y_val))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(optnm2preds_val_grd['abhishek'].squeeze()== y_val.squeeze()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(optnm2qwk, open(f'{p_o}/optnm2qwk.p', 'wb'))\n",
    "pickle.dump(optnm2coefs, open(f'{p_o}/optnm2coefs.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goes to Kernel!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = 'DevCv070116'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Making pretrained weights work without needing to find the default filename\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "!cp '../input/resnet50/resnet50.pth' '/tmp/.cache/torch/checkpoints/resnet50-19c8e357.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2grd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1])  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2grd = np.array(img2grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.all([Path(o[0]).exists() for o in img2grd]): print('All files are here!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_blnd_train = np.where(df.fnm.str.contains('aptos2019-blindness-detection/train_images'))[0]\n",
    "idx_val = np.random.choice(idx_blnd_train, len_blnd_test, replace=False)\n",
    "df['is_val']=False\n",
    "df.loc[idx_val, 'is_val']=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=True,flip_vert=True,\n",
    "                      max_rotate=360,max_warp=0,\n",
    "                      max_zoom=1.1,max_lighting=0.1,p_lighting=0.5)\n",
    "\n",
    "def get_data(sz, bs):\n",
    "    src = (ImageList.from_df(df=df,path='./',cols='fnm') #get dataset from dataset\n",
    "            .split_from_df(col='is_val') #Splitting the dataset\n",
    "            .label_from_df(cols='target',  label_cls=FloatList) #obtain labels from the level column\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms,size=sz,\n",
    "                         resize_method=ResizeMethod.SQUISH,\n",
    "                         padding_mode='zeros') #Data augmentation\n",
    "            .databunch(bs=bs,num_workers=0) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128 #smaller batch size is better for training, but may take longer\n",
    "sz = 224\n",
    "data = get_data(sz, bs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data.show_batch(rows=3, figsize=(7,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, base_arch=models.resnet50, \n",
    "                    metrics = [mse], path=p_o)\n",
    "learn.loss = MSELossFlat\n",
    "learn = learn.load('mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.add_test(ImageList.from_df(sample_df,\n",
    "                                      '../input/aptos2019-blindness-detection',\n",
    "                                      folder='test_images',\n",
    "                                      suffix='.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predictions for test set\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds_tst =  preds_tst.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket(preds_raw, coef = [0.5, 1.5, 2.5, 3.5]):\n",
    "    preds = np.zeros(preds_raw.shape)\n",
    "    for i, pred in enumerate(preds_raw):\n",
    "        if pred < coef[0]:\n",
    "            preds[i] = 0\n",
    "        elif pred >= coef[0] and pred < coef[1]:\n",
    "            preds[i] = 1\n",
    "        elif pred >= coef[1] and pred < coef[2]:\n",
    "            preds[i] = 2\n",
    "        elif pred >= coef[2] and pred < coef[3]:\n",
    "            preds[i] = 3\n",
    "        else:\n",
    "            preds[i] = 4\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2qwk = pickle.load(open(f'{p_o}/optnm2qwk.p','rb'))\n",
    "optnm2coefs = pickle.load(open(f'{p_o}/optnm2coefs.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optnm2qwk, optnm2coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = optnm2coefs['abhishek']\n",
    "preds_tst_grd = bucket(preds_tst, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_tst_grd.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(preds_tst_grd.squeeze()).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "sample.diagnosis = preds_tst_grd.squeeze().astype(int)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_df.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
