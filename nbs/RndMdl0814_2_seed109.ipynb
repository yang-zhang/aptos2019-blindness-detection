{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = f'RndMdl0814_2_seed{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'\n",
    "\n",
    "# p_o = f'.'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = False\n",
    "if dbg: dbgsz=500\n",
    "\n",
    "from fastai.vision import * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "source": [
    "!pip install ../input/efficientnetpytorch/efficientnet_pytorch-0.3.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 15 14:09:10 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    41W / 300W |     10MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading: \"http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\" to /tmp/.cache/torch/checkpoints/efficientnet-b3-c8376fa2.pth\n",
    "import os\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "\n",
    "!cp ../input/efficientnetpytorch/*.pth /tmp/.cache/torch/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 456\n"
     ]
    }
   ],
   "source": [
    "BS = 16\n",
    "FP16 = True\n",
    "PERC_VAL = 0.1\n",
    "WD = 0.01\n",
    "\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_open_yz = True\n",
    "\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "def load_ben_color(fn)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = crop_image_from_gray(image)\n",
    "    image, _ = crop_margin(image)\n",
    "    image = center_crop(image)\n",
    "    image = cv2.resize(image, (640, 480))#most common in test\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) , -4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfms = dict(\n",
    "    do_flip=True,\n",
    "    flip_vert=True,\n",
    "    max_rotate=360,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "    \n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "    \n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = load_ben_color(fn)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "    \n",
    "if use_open_yz:\n",
    "    vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "#         quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        quadratic_weighted_kappa(np.argmax(y_pred,1), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    aug_tfms = [o for o in learn.data.train_ds.tfms if o.tfm !=zoom]\n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "t2c_trn_has = dict(cnt)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/diabetic-retinopathy-resized'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'trainLabels.csv')\n",
    "img2grd_diab = [(f'{p}/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "# img2grd_diab = [(f'{p}/resized_train/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38788, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15287, '../input/diabetic-retinopathy-resized/resized_train/14598_right.jpeg', 0, 'diab'],\n",
       "       [7992, '../input/diabetic-retinopathy-resized/resized_train/5453_left.jpeg', 0, 'diab'],\n",
       "       [6506, '../input/diabetic-retinopathy-resized/resized_train/3531_left.jpeg', 0, 'diab'],\n",
       "       [25779, '../input/diabetic-retinopathy-resized/resized_train/27889_right.jpeg', 2, 'diab'],\n",
       "       [21737, '../input/diabetic-retinopathy-resized/resized_train/22775_right.jpeg', 0, 'diab']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27615\n",
       "2     6291\n",
       "1     2813\n",
       "3     1066\n",
       "4     1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use = df[df.src=='blnd'].copy()\n",
    "\n",
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 506, 3: 461, 4: 548, 1: 550}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randint(low=300, high=900):\n",
    "    res = np.random.randn()*300+600\n",
    "    return int(min(max(low, res), high))\n",
    "\n",
    "# set_torch_seed()\n",
    "n_t_extra = {2:get_randint(),3:get_randint(),4:get_randint(),1:get_randint()}\n",
    "n_t_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "for t,n in n_t_extra.items():\n",
    "    df_t_diab = df[(df.target==t) & (df.src=='diab')]\n",
    "    df2use = pd.concat([df2use, df_t_diab.sample(min(n, len(df_t_diab)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5727, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2    1505\n",
       "1     920\n",
       "4     843\n",
       "3     654\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: \n",
    "    df2use = df2use.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.75 s, sys: 95.1 ms, total: 4.84 s\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz=SZ, bs=BS):\n",
    "    src = (ImageList.from_df(df=df2use,path='./',cols='fnm') \n",
    "#             .split_by_rand_pct(0.2) \n",
    "            .split_none()\n",
    "            .label_from_df(cols='target',  \n",
    "                           #label_cls=FloatList\n",
    "                          )\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=sz,\n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "\n",
    "set_torch_seed()\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: test = test.head(dbgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test,\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME, num_classes=5) \n",
    "learn = Learner(data, model, path=p_o, \n",
    "#                 wd=WD,  \n",
    "#                 metrics=[accuracy, qwk],\n",
    "               )\n",
    "if FP16: learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "learn.recorder.plot(suggestion=True, skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.823318</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.828533</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.770663</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.694475</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.650670</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.617671</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.525059</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.466675</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.478988</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "learn.fit_one_cycle(10, max_lr=1e-3, \n",
    "#                     callbacks=[SaveModelCallback(learn, \n",
    "#                                                  every='epoch', \n",
    "#                                                  name=f'{PRFX}_model')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'rndmdl_seed_{SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 4.48 s, total: 17.2 s\n",
      "Wall time: 49.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds_tst = preds_tst.numpy().squeeze()\n",
    "preds_tst = np.argmax(preds_tst, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst_tta, _ = learn.TTA(ds_type=DatasetType.Test)\n",
    "preds_tst_tta = preds_tst_tta.numpy().squeeze()\n",
    "preds_tst_tta = np.argmax(preds_tst_tta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1273\n",
       "0     327\n",
       "1     132\n",
       "4     107\n",
       "3      89\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds_tst.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.Series(preds_tst_tta.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          4\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "subm['diagnosis'] = preds_tst\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1273\n",
       "0     327\n",
       "1     132\n",
       "4     107\n",
       "3      89\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
