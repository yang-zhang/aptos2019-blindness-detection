{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX = f'RndMdl0814_2_seed{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = f'../output/{PRFX}'\n",
    "\n",
    "# p_o = f'.'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(p_o).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = False\n",
    "if dbg: dbgsz=500\n",
    "\n",
    "from fastai.vision import * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "source": [
    "!pip install ../input/efficientnetpytorch/efficientnet_pytorch-0.3.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 15 04:27:00 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    41W / 300W |     10MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading: \"http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\" to /tmp/.cache/torch/checkpoints/efficientnet-b3-c8376fa2.pth\n",
    "import os\n",
    "if not os.path.exists('/tmp/.cache/torch/checkpoints/'):\n",
    "        os.makedirs('/tmp/.cache/torch/checkpoints/')\n",
    "\n",
    "!cp ../input/efficientnetpytorch/*.pth /tmp/.cache/torch/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b0 size 224\n",
      "efficientnet-b1 size 240\n",
      "efficientnet-b2 size 260\n",
      "efficientnet-b3 size 300\n",
      "efficientnet-b4 size 380\n",
      "efficientnet-b5 size 456\n",
      "SZ: 456\n"
     ]
    }
   ],
   "source": [
    "BS = 16\n",
    "FP16 = True\n",
    "PERC_VAL = 0.1\n",
    "WD = 0.01\n",
    "\n",
    "\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "SZ = EfficientNet.get_image_size(MODEL_NAME)\n",
    "for i in range(6):\n",
    "    print(f'efficientnet-b{i} size', EfficientNet.get_image_size(f'efficientnet-b{i}'))\n",
    "print('SZ:', SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_open_yz = True\n",
    "\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "def load_ben_color(fn)->Image:\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = crop_image_from_gray(image)\n",
    "    image, _ = crop_margin(image)\n",
    "    image = center_crop(image)\n",
    "    image = cv2.resize(image, (640, 480))#most common in test\n",
    "#     image = cv2.resize(image, (SZ, SZ))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX=10) , -4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) â†’ Collection[Transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfms = dict(\n",
    "    do_flip=True,\n",
    "    flip_vert=True,\n",
    "    max_rotate=360,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, the library resizes the image while keeping its original ratio so that the smaller size corresponds to the given size, then takes a crop (ResizeMethod.CROP). You can choose to resize the image while keeping its original ratio so that the bigger size corresponds to the given size, then take a pad (ResizeMethod.PAD). Another way is to just squish the image to the given size (ResizeMethod.SQUISH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_tfms = dict(\n",
    "    resize_method=ResizeMethod.SQUISH,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "set_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_margin(image, keep_less=0.83):\n",
    "    \n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        #print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    r = r*keep_less\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        #print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "\n",
    "    \n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "    \n",
    "# https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def center_crop(img):        \n",
    "    \n",
    "    h0, w0 = 480, 640 #most common in test\n",
    "    ratio = h0/w0 #most common in test\n",
    "    height, width, _= img.shape\n",
    "    new_width, new_height = width, math.ceil(width*ratio)\n",
    "\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "\n",
    "    if new_width is None:\n",
    "        new_width = min(width, height)\n",
    "\n",
    "    if new_height is None:\n",
    "        new_height = min(width, height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width) / 2))\n",
    "    right = width - int(np.floor((width - new_width) / 2))\n",
    "\n",
    "    top = int(np.ceil((height - new_height) / 2))\n",
    "    bottom = height - int(np.floor((height - new_height) / 2))\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        center_cropped_img = img[top:bottom, left:right]\n",
    "    else:\n",
    "        center_cropped_img = img[top:bottom, left:right, ...]\n",
    "\n",
    "    return center_cropped_img\n",
    "\n",
    "def open_yz(fn, convert_mode, after_open)->Image:\n",
    "    image = load_ben_color(fn)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "    \n",
    "if use_open_yz:\n",
    "    vision.data.open_image = open_yz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def quadratic_weighted_kappa(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "\n",
    "def qwk(y_pred, y):\n",
    "    return torch.tensor(\n",
    "#         quadratic_weighted_kappa(torch.round(y_pred), y),\n",
    "        quadratic_weighted_kappa(np.argmax(y_pred,1), y),\n",
    "        device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    aug_tfms = [o for o in learn.data.train_ds.tfms if o.tfm !=zoom]\n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=5, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3662, 1928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2grd = []\n",
    "\n",
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'train.csv')\n",
    "test  = pd.read_csv(pp/'test.csv')\n",
    "len_blnd = len(train)\n",
    "len_blnd_test = len(test)\n",
    "\n",
    "img2grd_blnd = [(f'{p}/train_images/{o[0]}.png',o[1],'blnd')  for o in train.values]\n",
    "\n",
    "len_blnd, len_blnd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1805), (2, 999), (1, 370), (4, 295), (3, 193)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.4929000546149645),\n",
       " (2, 0.272801747678864),\n",
       " (1, 0.1010376843255052),\n",
       " (4, 0.08055707263790278),\n",
       " (3, 0.052703440742763515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2grd += img2grd_blnd\n",
    "display(len(img2grd))\n",
    "cnt = Counter(o[1] for o in img2grd)\n",
    "t2c_trn_has = dict(cnt)\n",
    "display(cnt.most_common())\n",
    "sm = sum(cnt.values())\n",
    "display([(o[0], o[1]/sm) for o in cnt.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/diabetic-retinopathy-resized'\n",
    "pp = Path(p)\n",
    "train = pd.read_csv(pp/'trainLabels.csv')\n",
    "img2grd_diab = [(f'{p}/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "# img2grd_diab = [(f'{p}/resized_train/resized_train/{o[0]}.jpeg',o[1],'diab')  for o in train.values]\n",
    "img2grd += img2grd_diab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38788, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(img2grd)\n",
    "df.columns = ['fnm', 'target', 'src']\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34144, '../input/diabetic-retinopathy-resized/resized_train/38532_left.jpeg', 0, 'diab'],\n",
       "       [24608, '../input/diabetic-retinopathy-resized/resized_train/26383_left.jpeg', 0, 'diab'],\n",
       "       [14701, '../input/diabetic-retinopathy-resized/resized_train/13867_right.jpeg', 0, 'diab'],\n",
       "       [2888, '../input/aptos2019-blindness-detection/train_images/c8905b8d5cf1.png', 2, 'blnd'],\n",
       "       [19096, '../input/diabetic-retinopathy-resized/resized_train/19331_left.jpeg', 0, 'diab']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all([Path(o[0]).exists() for o in img2grd]): print('Some files are missing!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27615\n",
       "2     6291\n",
       "1     2813\n",
       "3     1066\n",
       "4     1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use = df[df.src=='blnd'].copy()\n",
    "\n",
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 495, 3: 900, 4: 900, 1: 827}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randint(low=300, high=900):\n",
    "    res = np.random.randn()*300+600\n",
    "    return int(min(max(low, res), high))\n",
    "\n",
    "# set_torch_seed()\n",
    "n_t_extra = {2:get_randint(),3:get_randint(),4:get_randint(),1:get_randint()}\n",
    "n_t_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_seed()\n",
    "for t,n in n_t_extra.items():\n",
    "    df_t_diab = df[(df.target==t) & (df.src=='diab')]\n",
    "    df2use = pd.concat([df2use, df_t_diab.sample(min(n, len(df_t_diab)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6565, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2    1494\n",
       "1    1197\n",
       "3    1066\n",
       "4    1003\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2use.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: \n",
    "    df2use = df2use.head(dbgsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.95 s, sys: 157 ms, total: 5.11 s\n",
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfms = get_transforms(**params_tfms)\n",
    "\n",
    "def get_data(sz=SZ, bs=BS):\n",
    "    src = (ImageList.from_df(df=df2use,path='./',cols='fnm') \n",
    "#             .split_by_rand_pct(0.2) \n",
    "            .split_none()\n",
    "            .label_from_df(cols='target',  \n",
    "                           #label_cls=FloatList\n",
    "                          )\n",
    "          )\n",
    "\n",
    "    data= (src.transform(tfms, size=sz,\n",
    "                         **kwargs_tfms\n",
    "                         ) #Data augmentation\n",
    "            .databunch(bs=bs) #DataBunch\n",
    "            .normalize(imagenet_stats) #Normalize     \n",
    "           )\n",
    "    return data\n",
    "\n",
    "\n",
    "set_torch_seed()\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../input/aptos2019-blindness-detection'\n",
    "pp = Path(p)\n",
    "test  = pd.read_csv(pp/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dbg: test = test.head(dbgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_test(ImageList.from_df(test,\n",
    "                                '../input/aptos2019-blindness-detection',\n",
    "                                folder='test_images',\n",
    "                                suffix='.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "data.show_batch(rows=3, figsize=(10, 10), ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME, num_classes=5) \n",
    "learn = Learner(data, model, path=p_o, \n",
    "#                 wd=WD,  \n",
    "#                 metrics=[accuracy, qwk],\n",
    "               )\n",
    "if FP16: learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "learn.recorder.plot(suggestion=True, skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.808336</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.861174</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.826252</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.791963</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.730789</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.665938</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.599736</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.578983</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.508511</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.453269</td>\n",
       "      <td>#na#</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_torch_seed()\n",
    "learn.fit_one_cycle(10, max_lr=1e-3, \n",
    "#                     callbacks=[SaveModelCallback(learn, \n",
    "#                                                  every='epoch', \n",
    "#                                                  name=f'{PRFX}_model')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'rndmdl_seed_{SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 4.4 s, total: 17.1 s\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst, _ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds_tst = preds_tst.numpy().squeeze()\n",
    "preds_tst = np.argmax(preds_tst, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "set_torch_seed()\n",
    "preds_tst_tta, _ = learn.TTA(ds_type=DatasetType.Test)\n",
    "preds_tst_tta = preds_tst_tta.numpy().squeeze()\n",
    "preds_tst_tta = np.argmax(preds_tst_tta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1155\n",
       "0     343\n",
       "1     197\n",
       "4     119\n",
       "3     114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds_tst.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.Series(preds_tst_tta.astype(int)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          4\n",
       "2  006efc72b638          3\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "subm['diagnosis'] = preds_tst\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1155\n",
       "0     343\n",
       "1     197\n",
       "4     119\n",
       "3     114\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv(f\"{p_o}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
